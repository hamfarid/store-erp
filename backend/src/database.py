"""
Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø©
Enhanced Database Configuration
"""

import os
import logging
from flask_sqlalchemy import SQLAlchemy
from flask_migrate import Migrate
from datetime import datetime

# Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
db = SQLAlchemy()
migrate = Migrate()

logger = logging.getLogger(__name__)


def clear_test_database():
    """
    Clear and recreate database for testing between test runs.
    Ensures test isolation by providing a fresh database.
    """
    try:
        db.session.remove()
        db.drop_all()
        db.create_all()
        logger.debug("âœ“ Test database cleared and recreated")
        return True
    except Exception as e:
        logger.error(f"âŒ Error clearing test database: {e}")
        return False


def configure_database(app):
    """ØªÙƒÙˆÙŠÙ† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Flask app"""

    # Check for DATABASE_URL from environment (Docker/Production)
    # This allows Docker Compose to use PostgreSQL while development uses SQLite
    database_url = os.environ.get("DATABASE_URL")

    if database_url:
        app.config["SQLALCHEMY_DATABASE_URI"] = database_url

        if database_url.startswith("sqlite:"):
            logger.info("âœ… Using SQLite from DATABASE_URL environment variable")

            # If this is a file-backed SQLite DB, ensure parent directory exists.
            # Examples:
            # - sqlite:///:memory:
            # - sqlite:///D:/path/to/db.sqlite
            # - sqlite:////absolute/path/to/db.sqlite
            if ":memory:" not in database_url:
                url_no_query = database_url.split("?", 1)[0]
                if url_no_query.startswith("sqlite:////"):
                    db_path = url_no_query[len("sqlite:////") :]
                elif url_no_query.startswith("sqlite:///"):
                    db_path = url_no_query[len("sqlite:///") :]
                else:
                    db_path = ""

                if db_path:
                    db_path = os.path.normpath(db_path.replace("/", os.sep))
                    parent_dir = os.path.dirname(db_path)
                    if parent_dir:
                        os.makedirs(parent_dir, exist_ok=True)

            # SQLite-specific engine options
            app.config["SQLALCHEMY_ENGINE_OPTIONS"] = {
                "pool_pre_ping": True,
                "pool_recycle": 300,
            }
        else:
            # Use PostgreSQL (Docker/Production)
            logger.info("âœ… Using PostgreSQL from DATABASE_URL environment variable")
            # PostgreSQL-specific engine options
            app.config["SQLALCHEMY_ENGINE_OPTIONS"] = {
                "pool_pre_ping": True,
                "pool_recycle": 300,
                "pool_size": 10,
                "max_overflow": 20,
            }
    else:
        # Fallback to SQLite for development
        basedir = os.path.abspath(os.path.dirname(__file__))
        instance_dir = os.path.join(os.path.dirname(basedir), "instance")

        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ instance Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        if not os.path.exists(instance_dir):
            os.makedirs(instance_dir)

        # Ù…Ø³Ø§Ø± Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        database_path = os.path.join(instance_dir, "inventory.db")
        app.config["SQLALCHEMY_DATABASE_URI"] = f"sqlite:///{database_path}"
        logger.info(f"âœ… Using SQLite for development: {database_path}")
        # SQLite-specific engine options
        app.config["SQLALCHEMY_ENGINE_OPTIONS"] = {
            "pool_pre_ping": True,
            "pool_recycle": 300,
        }

    # Common configuration for both database types
    app.config["SQLALCHEMY_TRACK_MODIFICATIONS"] = False

    # ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
    db.init_app(app)
    migrate.init_app(app, db)

    return db


def create_tables(app):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„"""
    import logging

    logger = logging.getLogger(__name__)

    try:
        with app.app_context():
            # Preload all models so SQLAlchemy metadata includes all FK targets before create_all()
            # IMPORTANT: Load models in dependency order (base tables first)
            logger.info("ğŸ”„ Loading models in dependency order...")

            try:
                # Phase 1: Base tables (no FK dependencies)
                logger.debug("Phase 1: Loading base models...")
                try:
                    from src.models.user import User, Role  # noqa: F401
                except ImportError:
                    from models.user import User, Role  # noqa: F401

                try:
                    from src.models.sales_engineer import SalesEngineer  # noqa: F401
                except ImportError:
                    try:
                        from models.sales_engineer import SalesEngineer  # noqa: F401
                    except ImportError:
                        logger.warning("âš ï¸ SalesEngineer model not available")

                try:
                    from src.models.customer import Customer  # noqa: F401
                except ImportError:
                    from models.customer import Customer  # noqa: F401

                try:
                    from src.models.supplier import Supplier  # noqa: F401
                except ImportError:
                    from models.supplier import Supplier  # noqa: F401
                logger.debug("âœ“ Base models loaded")

                # Phase 2: Inventory base tables
                logger.debug("Phase 2: Loading inventory models...")
                try:
                    from src.models.inventory import (
                        Category,
                        Product,
                        Warehouse,
                    )  # noqa: F401
                except ImportError:
                    from models.inventory import (
                        Category,
                        Product,
                        Warehouse,
                    )  # noqa: F401
                logger.debug("âœ“ Inventory models loaded")

                # Phase 3: Enhanced models - DISABLED to avoid schema conflicts
                # enhanced_models.py redefines Category with extra fields (name_ar, etc)
                # which causes schema mismatches with inventory.py Category
                logger.debug("Phase 3: Skipping enhanced models (schema conflicts)")

                # Phase 4: Invoice models
                logger.debug("Phase 4: Loading invoice models...")
                try:
                    from src.models.unified_invoice import (  # noqa: F401
                        UnifiedInvoice,
                        UnifiedInvoiceItem,
                    )
                    from src.models.invoice_unified import InvoicePayment  # noqa: F401

                    logger.debug("âœ“ Invoice models loaded")
                except ImportError as e:
                    logger.warning(f"âš ï¸ Invoice models not available: {e}")

                # Phase 5: Advanced sales models (now enabled with FK support)
                logger.debug("Phase 5: Loading advanced sales models...")
                try:
                    try:
                        from src.models.sales_advanced import (  # noqa: F401
                            SalesInvoice,
                            SalesInvoiceItem,
                            CustomerPayment,
                        )
                    except ImportError:
                        from models.sales_advanced import (  # noqa: F401
                            SalesInvoice,
                            SalesInvoiceItem,
                            CustomerPayment,
                        )
                    logger.debug("âœ“ Advanced sales models loaded")
                except Exception as sales_err:
                    logger.warning(f"âš ï¸ Advanced sales skipped: {sales_err}")

            except Exception as e:
                logger.error(f"âŒ Model preload error: {e}")
                logger.warning("âš ï¸ Continuing with partial initialization")

            # Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
            logger.info("ğŸ”„ Creating all database tables...")
            db.create_all()
            logger.info("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯Ø§ÙˆÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­")
            print("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯Ø§ÙˆÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­")

            # Return models to pass to create_default_data (avoid duplicate imports)
            return True, User, Role, Category, Warehouse
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„: {e}", exc_info=True)
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„: {e}")
        return False


def create_default_data(User=None, Role=None, Category=None, Warehouse=None):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©

    Args:
        User, Role, Category, Warehouse: Model classes passed from init_database()
            to avoid duplicate imports/registrations
    """
    from flask import has_app_context
    from sqlalchemy import select, func, inspect
    import logging

    logger = logging.getLogger(__name__)

    try:
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ app context
        if not has_app_context():
            print("âš ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Flask app contextØŒ ØªØ®Ø·ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©")
            return True

        print("DEBUG: Starting create_default_data()...")
        print(
            f"DEBUG: Received models - User: {User}, Role: {Role}, Category: {Category}, Warehouse: {Warehouse}"
        )

        # Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙ‚Ø· Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªÙ…Ø±ÙŠØ±Ù‡Ø§ (Ù„Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©)
        # âš ï¸ WARNING: Avoid this path when called from init_database()
        if User is None or Role is None or Category is None or Warehouse is None:
            print("âš ï¸ DEBUG: Models not passed, importing...")
            try:
                from src.models.user import User, Role
                from src.models.inventory import Category, Warehouse
            except ImportError:
                from models.user import User, Role
                from models.inventory import Category, Warehouse
            print("DEBUG: Models imported successfully")
        else:
            print("âœ“ DEBUG: Using passed models")

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ø¯ÙˆØ§Ø± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        # Query using raw SQL to avoid mapper configuration conflicts
        role_count = db.session.execute(db.text("SELECT COUNT(*) FROM roles")).scalar()

        if role_count == 0:
            inspector = inspect(db.engine)
            role_columns = {c["name"] for c in inspector.get_columns("roles")}

            role_rows = [
                {
                    "code": "admin",
                    "name": "admin",
                    "name_ar": "Ù…Ø¯ÙŠØ± Ø§Ù„Ù†Ø¸Ø§Ù…",
                    "description": "Ù…Ø¯ÙŠØ± Ø§Ù„Ù†Ø¸Ø§Ù…",
                    "description_ar": "Ù…Ø¯ÙŠØ± Ø§Ù„Ù†Ø¸Ø§Ù…",
                    "is_active": 1,
                    "is_system": 1,
                },
                {
                    "code": "user",
                    "name": "user",
                    "name_ar": "Ù…Ø³ØªØ®Ø¯Ù… Ø¹Ø§Ø¯ÙŠ",
                    "description": "Ù…Ø³ØªØ®Ø¯Ù… Ø¹Ø§Ø¯ÙŠ",
                    "description_ar": "Ù…Ø³ØªØ®Ø¯Ù… Ø¹Ø§Ø¯ÙŠ",
                    "is_active": 1,
                    "is_system": 1,
                },
            ]

            # Only insert columns that actually exist in the current schema.
            insert_columns = [
                c
                for c in [
                    "code",
                    "name",
                    "name_ar",
                    "description",
                    "description_ar",
                    "is_active",
                    "is_system",
                ]
                if c in role_columns
            ]
            if not insert_columns:
                raise RuntimeError(
                    "roles table has no recognized columns for default insert"
                )

            values_sql = []
            params = {}
            for i, row in enumerate(role_rows):
                placeholders = []
                for col in insert_columns:
                    key = f"{col}_{i}"
                    placeholders.append(f":{key}")
                    params[key] = row.get(col)
                values_sql.append(f"({', '.join(placeholders)})")

            sql = (
                f"INSERT INTO roles ({', '.join(insert_columns)}) VALUES "
                + ", ".join(values_sql)
            )
            db.session.execute(db.text(sql), params)
            db.session.commit()
            print("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ø¯ÙˆØ§Ø± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©")

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠ
        # Query using raw SQL
        admin_exists = db.session.execute(
            db.text("SELECT COUNT(*) FROM users WHERE username = 'admin'")
        ).scalar()

        if admin_exists == 0:
            # Hash password using bcrypt directly
            import bcrypt

            password_hash = bcrypt.hashpw(
                "admin123".encode(), bcrypt.gensalt()
            ).decode()

            db.session.execute(
                db.text(
                    """
                INSERT INTO users (username, email, full_name, password_hash, role_id, is_active)
                VALUES ('admin', 'admin@store.com', 'Ù…Ø¯ÙŠØ± Ø§Ù„Ù†Ø¸Ø§Ù…', :pwd, 1, 1)
            """
                ),
                {"pwd": password_hash},
            )
            db.session.commit()
            print("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠ")

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        category_count = db.session.execute(
            db.text("SELECT COUNT(*) FROM categories")
        ).scalar()

        if category_count == 0:
            db.session.execute(
                db.text(
                    """
                INSERT INTO categories (name, description) VALUES
                ('Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª', 'Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ©'),
                ('Ù…Ù„Ø§Ø¨Ø³', 'Ø§Ù„Ù…Ù„Ø§Ø¨Ø³ ÙˆØ§Ù„Ø£Ø²ÙŠØ§Ø¡'),
                ('Ø·Ø¹Ø§Ù…', 'Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ØºØ°Ø§Ø¦ÙŠØ©'),
                ('ÙƒØªØ¨', 'Ø§Ù„ÙƒØªØ¨ ÙˆØ§Ù„Ù…Ø·Ø¨ÙˆØ¹Ø§Øª'),
                ('Ø£Ø¯ÙˆØ§Øª', 'Ø§Ù„Ø£Ø¯ÙˆØ§Øª ÙˆØ§Ù„Ù…Ø¹Ø¯Ø§Øª')
            """
                )
            )
            db.session.commit()
            print("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©")

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø®Ø§Ø²Ù† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        warehouse_count = db.session.execute(
            db.text("SELECT COUNT(*) FROM warehouses")
        ).scalar()

        if warehouse_count == 0:
            db.session.execute(
                db.text(
                    """
                INSERT INTO warehouses (name, code, address, is_active) VALUES
                ('Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ', 'WH001', 'Ø§Ù„Ø±ÙŠØ§Ø¶ - Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ø´Ø±ÙƒØ©', 1),
                ('Ù…Ø®Ø²Ù† ÙØ±Ø¹ÙŠ', 'WH002', 'Ø¬Ø¯Ø© - Ù…Ø®Ø²Ù† ÙØ±Ø¹ÙŠ', 1)
            """
                )
            )
            db.session.commit()
            print("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø®Ø§Ø²Ù† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©")

        print("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­")
        return True

    except Exception as e:
        import traceback

        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {e}")
        print("Full traceback:")
        traceback.print_exc()
        try:
            db.session.rollback()
        except Exception:  # noqa: BLE001
            pass
        return False


def get_database_info():
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    try:
        info = {"tables": [], "total_records": 0}

        # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
        tables = db.metadata.tables.keys()
        info["tables"] = list(tables)

        # Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª ÙÙŠ ÙƒÙ„ Ø¬Ø¯ÙˆÙ„
        from sqlalchemy import text

        for table_name in tables:
            try:
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… text() Ù„ØªØ¬Ù†Ø¨ ØªØ­Ø°ÙŠØ± SQL
                count_query = text(f"SELECT COUNT(*) FROM {table_name}")
                count = db.session.execute(count_query).scalar()
                info[f"{table_name}_count"] = count
                info["total_records"] += count
            except BaseException:
                info[f"{table_name}_count"] = 0

        return info

    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        return {}


def backup_database():
    """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    try:
        import shutil
        from datetime import datetime

        basedir = os.path.abspath(os.path.dirname(__file__))
        instance_dir = os.path.join(os.path.dirname(basedir), "instance")

        source_db = os.path.join(instance_dir, "inventory.db")
        backup_name = f"inventory_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.db"
        backup_path = os.path.join(instance_dir, backup_name)

        if os.path.exists(source_db):
            shutil.copy2(source_db, backup_path)
            print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {backup_name}")
            return backup_path
        else:
            print("âŒ Ù…Ù„Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
            return None

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {e}")
        return None


def optimize_database():
    """ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    from sqlalchemy import text

    try:
        # ØªØ´ØºÙŠÙ„ VACUUM Ù„ØªØ­Ø³ÙŠÙ† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        db.session.execute(text("VACUUM;"))

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
        db.session.execute(text("ANALYZE;"))

        db.session.commit()
        print("âœ… ØªÙ… ØªØ­Ø³ÙŠÙ† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        return True

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        return False


# Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„ØªØ·ÙˆÙŠØ±
def reset_database():
    """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ù„Ù„ØªØ·ÙˆÙŠØ± ÙÙ‚Ø·)"""
    try:
        db.drop_all()
        db.create_all()
        create_default_data()
        print("âœ… ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        return True
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        return False


def check_database_health():
    """ÙØ­Øµ ØµØ­Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    from sqlalchemy import text

    try:
        # ÙØ­Øµ Ø§Ù„Ø§ØªØµØ§Ù„
        db.session.execute(text("SELECT 1;"))

        # ÙØ­Øµ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        required_tables = ["users", "roles", "products", "categories", "warehouses"]
        existing_tables = db.metadata.tables.keys()

        missing_tables = [
            table for table in required_tables if table not in existing_tables
        ]

        health_status = {
            "connection": True,
            "tables_exist": len(missing_tables) == 0,
            "missing_tables": missing_tables,
            "total_tables": len(existing_tables),
        }

        return health_status

    except Exception as e:
        return {
            "connection": False,
            "error": str(e),
            "tables_exist": False,
            "missing_tables": [],
            "total_tables": 0,
        }
