"""
Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù: /home/ubuntu/gaara_scan_ai_final_4.2/src/modules/plant_disease/model_benchmark_system.py

Ù†Ø¸Ø§Ù… Ø´Ø§Ù…Ù„ Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙˆÙ…Ù‚Ø§Ø±Ù†Ø© Ù†Ù…Ø§Ø°Ø¬ ØªØ´Ø®ÙŠØµ Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ù†Ø¨Ø§ØªØ§Øª
ÙŠÙˆÙØ± Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù… Ø£Ø¯ÙˆØ§Øª Ù„Ù‚ÙŠØ§Ø³ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø­ÙŠØ« Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
ÙˆÙŠÙ‚Ø¯Ù… ØªÙ‚Ø§Ø±ÙŠØ± Ù…ÙØµÙ„Ø© ÙˆÙ…Ø±Ø¦ÙŠØ© Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙÙŠ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù„Ø§Ø­ØªÙŠØ§Ø¬Ø§ØªÙ‡Ù…
"""

import time
import psutil
import torch
import numpy as np
import matplotlib.pyplot as plt

from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import pandas as pd
import threading
import gc
import json
import os
import warnings
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional

# ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª
warnings.filterwarnings('ignore')


class ModelBenchmarkSystem:
    """Ù†Ø¸Ø§Ù… Ø´Ø§Ù…Ù„ Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙˆÙ…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""

    def __init__(self, processor):
        """
        ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… Ø§Ø®ØªØ¨Ø§Ø± ÙˆÙ…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬

        Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª:
            processor: Ù…Ø¹Ø§Ù„Ø¬ ØªØ´Ø®ÙŠØµ Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ù†Ø¨Ø§ØªØ§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
        """
        self.processor = processor
        self.benchmark_results = {}
        self.performance_metrics = {}
        self.resource_usage = {}
        self.reports_dir = Path("/home/ubuntu/gaara_scan_ai_final_4.2/reports/benchmarks")

        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        os.makedirs(self.reports_dir, exist_ok=True)

    def run_comprehensive_benchmark(self, test_images_path: List[str], ground_truth_labels: List[int],
                                    models_to_test: Optional[List[str]] = None, num_iterations: int = 3) -> Dict[str, Any]:
        """
        ØªØ´ØºÙŠÙ„ Ø§Ø®ØªØ¨Ø§Ø± Ø´Ø§Ù…Ù„ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬

        Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª:
            test_images_path: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØµÙˆØ± Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
            ground_truth_labels: Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ø§Ù„ØµØ­ÙŠØ­Ø© Ù„Ù„ØµÙˆØ±
            models_to_test: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± (Ø¥Ø°Ø§ ÙƒØ§Ù†Øª NoneØŒ ÙŠØªÙ… Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©)
            num_iterations: Ø¹Ø¯Ø¯ Ù…Ø±Ø§Øª ØªÙƒØ±Ø§Ø± Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù„ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬

        Ø§Ù„Ø¹Ø§Ø¦Ø¯:
            Ù‚Ø§Ù…ÙˆØ³ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù„ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬
        """

        if models_to_test is None:
            models_to_test = list(self.processor.models.keys())

        print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬...")
        print(f"ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {len(models_to_test)}")
        print(f"ğŸ–¼ï¸ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ±: {len(test_images_path)}")
        print(f"ğŸ”„ Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: {num_iterations}")

        results = {}

        for model_name in models_to_test:
            print(f"\nğŸ” Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {model_name}")

            # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
            model_results = []
            for iteration in range(num_iterations):
                print(f"  ğŸ“ˆ Ø§Ù„ØªÙƒØ±Ø§Ø± {iteration + 1}/{num_iterations}")

                result = self.benchmark_single_model(
                    model_name, test_images_path, ground_truth_labels, iteration
                )
                model_results.append(result)

                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª ÙˆØ§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            results[model_name] = self.aggregate_results(model_results)

        self.benchmark_results = results
        self.generate_comparison_report()
        self.create_visualizations()

        return results

    def benchmark_single_model(self, model_name: str, test_images: List[str],
                               ground_truth: List[int], iteration: int) -> Dict[str, Any]:
        """
        Ø§Ø®ØªØ¨Ø§Ø± Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ø­Ø¯

        Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª:
            model_name: Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
            test_images: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØµÙˆØ± Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
            ground_truth: Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ø§Ù„ØµØ­ÙŠØ­Ø© Ù„Ù„ØµÙˆØ±
            iteration: Ø±Ù‚Ù… Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ

        Ø§Ù„Ø¹Ø§Ø¦Ø¯:
            Ù‚Ø§Ù…ÙˆØ³ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
        """

        if model_name not in self.processor.models:
            return {"error": f"Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name} ØºÙŠØ± Ù…ØªØ§Ø­"}

        # Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
        resource_monitor = ResourceMonitor()
        resource_monitor.start_monitoring()

        # ØªØ³Ø¬ÙŠÙ„ ÙˆÙ‚Øª Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
        start_time = time.time()

        # Ù‚ÙŠØ§Ø³ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù‚Ø¨Ù„ Ø§Ù„ØªÙ†ÙÙŠØ°
        memory_before = psutil.virtual_memory().used / (1024**3)  # GB
        gpu_memory_before = self.get_gpu_memory() if torch.cuda.is_available() else 0

        # Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
        predictions = []
        confidences = []
        inference_times = []

        for i, image_path in enumerate(test_images):
            # Ù‚ÙŠØ§Ø³ ÙˆÙ‚Øª Ø§Ù„ØªÙ†Ø¨Ø¤ Ù„ÙƒÙ„ ØµÙˆØ±Ø©
            img_start_time = time.time()

            try:
                result = self.processor.predict_single_model(image_path, model_name)

                if "error" not in result:
                    predictions.append(result['prediction'])
                    confidences.append(result['confidence'])
                else:
                    predictions.append(-1)  # Ø®Ø·Ø£
                    confidences.append(0.0)

            except Exception as e:
                print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø© {i}: {e}")
                predictions.append(-1)
                confidences.append(0.0)

            img_end_time = time.time()
            inference_times.append(img_end_time - img_start_time)

            # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù…
            if (i + 1) % 10 == 0:
                print(f"    ğŸ“¸ ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {i + 1}/{len(test_images)} ØµÙˆØ±Ø©")

        # ØªØ³Ø¬ÙŠÙ„ ÙˆÙ‚Øª Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
        end_time = time.time()
        total_time = end_time - start_time

        # Ù‚ÙŠØ§Ø³ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†ÙÙŠØ°
        memory_after = psutil.virtual_memory().used / (1024**3)  # GB
        gpu_memory_after = self.get_gpu_memory() if torch.cuda.is_available() else 0

        # Ø¥ÙŠÙ‚Ø§Ù Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
        resource_stats = resource_monitor.stop_monitoring()

        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
        metrics = self.calculate_metrics(predictions, ground_truth, confidences)

        return {
            "model_name": model_name,
            "iteration": iteration,
            "predictions": predictions,
            "confidences": confidences,
            "metrics": metrics,
            "timing": {
                "total_time": total_time,
                "avg_inference_time": np.mean(inference_times),
                "min_inference_time": np.min(inference_times),
                "max_inference_time": np.max(inference_times),
                "fps": len(test_images) / total_time
            },
            "resource_usage": {
                "memory_used": memory_after - memory_before,
                "gpu_memory_used": gpu_memory_after - gpu_memory_before,
                "cpu_usage": resource_stats['avg_cpu_usage'],
                "peak_memory": resource_stats['peak_memory']
            },
            "inference_times": inference_times
        }

    def calculate_metrics(self, predictions: List[int], ground_truth: List[int],
                          confidences: List[float]) -> Dict[str, Any]:
        """
        Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡

        Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª:
            predictions: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
            ground_truth: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ø§Ù„ØµØ­ÙŠØ­Ø©
            confidences: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø«Ù‚Ø© Ù„Ù„ØªÙ†Ø¨Ø¤Ø§Øª

        Ø§Ù„Ø¹Ø§Ø¦Ø¯:
            Ù‚Ø§Ù…ÙˆØ³ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡
        """

        # ØªØµÙÙŠØ© Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„Ø®Ø§Ø·Ø¦Ø©
        valid_indices = [i for i, pred in enumerate(predictions) if pred != -1]

        if not valid_indices:
            return {
                "accuracy": 0.0,
                "precision": 0.0,
                "recall": 0.0,
                "f1_score": 0.0,
                "error_rate": 1.0,
                "avg_confidence": 0.0
            }

        valid_predictions = [predictions[i] for i in valid_indices]
        valid_ground_truth = [ground_truth[i] for i in valid_indices]
        valid_confidences = [confidences[i] for i in valid_indices]

        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        accuracy = accuracy_score(valid_ground_truth, valid_predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(
            valid_ground_truth, valid_predictions, average='weighted', zero_division=0
        )

        error_rate = 1.0 - (len(valid_indices) / len(predictions))
        avg_confidence = np.mean(valid_confidences)

        # Ù…ØµÙÙˆÙØ© Ø§Ù„Ø®Ù„Ø·
        cm = confusion_matrix(valid_ground_truth, valid_predictions)

        return {
            "accuracy": float(accuracy),
            "precision": float(precision),
            "recall": float(recall),
            "f1_score": float(f1),
            "error_rate": float(error_rate),
            "avg_confidence": float(avg_confidence),
            "confusion_matrix": cm.tolist(),
            "valid_predictions": len(valid_indices),
            "total_predictions": len(predictions)
        }

    def aggregate_results(self, model_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ØªØ¬Ù…ÙŠØ¹ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©

        Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª:
            model_results: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬

        Ø§Ù„Ø¹Ø§Ø¦Ø¯:
            Ù‚Ø§Ù…ÙˆØ³ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø¬Ù…Ø¹Ø©
        """

        if not model_results or all("error" in result for result in model_results):
            return {"error": "ÙØ´Ù„ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª"}

        # ØªØµÙÙŠØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØµØ­ÙŠØ­Ø©
        valid_results = [result for result in model_results if "error" not in result]

        if not valid_results:
            return {"error": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ ØµØ­ÙŠØ­Ø©"}

        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
        metrics_keys = ['accuracy', 'precision', 'recall', 'f1_score', 'error_rate', 'avg_confidence']
        timing_keys = ['total_time', 'avg_inference_time', 'fps']
        resource_keys = ['memory_used', 'gpu_memory_used', 'cpu_usage', 'peak_memory']

        aggregated = {
            "model_name": valid_results[0]["model_name"],
            "num_iterations": len(valid_results),
            "metrics": {},
            "timing": {},
            "resource_usage": {},
            "stability": {}
        }

        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ù…Ø¹ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        for key in metrics_keys:
            values = [result["metrics"][key] for result in valid_results if key in result["metrics"]]
            if values:
                aggregated["metrics"][key] = {
                    "mean": float(np.mean(values)),
                    "std": float(np.std(values)),
                    "min": float(np.min(values)),
                    "max": float(np.max(values))
                }

        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªÙˆÙ‚ÙŠØªØ§Øª
        for key in timing_keys:
            values = [result["timing"][key] for result in valid_results if key in result["timing"]]
            if values:
                aggregated["timing"][key] = {
                    "mean": float(np.mean(values)),
                    "std": float(np.std(values)),
                    "min": float(np.min(values)),
                    "max": float(np.max(values))
                }

        # ØªØ¬Ù…ÙŠØ¹ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
        for key in resource_keys:
            values = [result["resource_usage"][key] for result in valid_results if key in result["resource_usage"]]
            if values:
                aggregated["resource_usage"][key] = {
                    "mean": float(np.mean(values)),
                    "std": float(np.std(values)),
                    "min": float(np.min(values)),
                    "max": float(np.max(values))
                }

        # Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±
        accuracy_values = [result["metrics"]["accuracy"] for result in valid_results]
        timing_values = [result["timing"]["avg_inference_time"] for result in valid_results]

        aggregated["stability"] = {
            "accuracy_cv": float(np.std(accuracy_values) / np.mean(accuracy_values)) if accuracy_values else 0,
            "timing_cv": float(np.std(timing_values) / np.mean(timing_values)) if timing_values else 0,
            "consistency_score": 1.0 - float(np.std(accuracy_values)) if accuracy_values else 0
        }

        return aggregated

    def generate_comparison_report(self) -> None:
        """
        Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù…Ù‚Ø§Ø±Ù†Ø© Ø´Ø§Ù…Ù„
        """

        if not self.benchmark_results:
            print("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©")
            return

        print("\n" + "=" * 80)
        print("ğŸ“Š ØªÙ‚Ø±ÙŠØ± Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø´Ø§Ù…Ù„")
        print("=" * 80)

        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
        self.print_performance_table()

        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„Ù…ÙˆØ§Ø±Ø¯
        self.print_speed_resource_table()

        # Ø§Ù„ØªÙˆØµÙŠØ§Øª
        self.print_recommendations()

        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        self.save_detailed_report()

    def print_performance_table(self) -> None:
        """
        Ø·Ø¨Ø§Ø¹Ø© Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡
        """

        print("\nğŸ¯ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ (Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ø¬ÙˆØ¯Ø©)")
        print("-" * 80)

        data = []
        for model_name, results in self.benchmark_results.items():
            if "error" not in results:
                metrics = results["metrics"]
                stability = results["stability"]

                data.append({
                    "Ø§Ù„Ù†Ù…ÙˆØ°Ø¬": model_name,
                    "Ø§Ù„Ø¯Ù‚Ø© (%)": f"{metrics['accuracy']['mean']*100:.2f} Â± {metrics['accuracy']['std']*100:.2f}",
                    "F1-Score": f"{metrics['f1_score']['mean']:.3f}",
                    "Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ù…ØªÙˆØ³Ø·Ø©": f"{metrics['avg_confidence']['mean']:.3f}",
                    "Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±": f"{stability['consistency_score']:.3f}"
                })

        df = pd.DataFrame(data)
        print(df.to_string(index=False))

    def print_speed_resource_table(self) -> None:
        """
        Ø·Ø¨Ø§Ø¹Ø© Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„Ù…ÙˆØ§Ø±Ø¯
        """

        print("\nâš¡ Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯")
        print("-" * 80)

        data = []
        for model_name, results in self.benchmark_results.items():
            if "error" not in results:
                timing = results["timing"]
                resources = results["resource_usage"]

                data.append({
                    "Ø§Ù„Ù†Ù…ÙˆØ°Ø¬": model_name,
                    "FPS": f"{timing['fps']['mean']:.2f}",
                    "Ø²Ù…Ù† Ø§Ù„ØªÙ†Ø¨Ø¤ (Ø«Ø§Ù†ÙŠØ©)": f"{timing['avg_inference_time']['mean']:.3f}",
                    "Ø§Ù„Ø°Ø§ÙƒØ±Ø© (GB)": f"{resources['memory_used']['mean']:.2f}",
                    "Ù…Ø¹Ø§Ù„Ø¬ Ù…Ø¦ÙˆÙŠ (%)": f"{resources['cpu_usage']['mean']:.1f}"
                })

        df = pd.DataFrame(data)
        print(df.to_string(index=False))

    def print_recommendations(self) -> None:
        """
        Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„ØªÙˆØµÙŠØ§Øª
        """

        print("\nğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª")
        print("-" * 80)

        if not self.benchmark_results:
            return

        # Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø¯Ù‚Ø©
        best_accuracy_model = max(
            self.benchmark_results.items(),
            key=lambda x: x[1]["metrics"]["accuracy"]["mean"] if "error" not in x[1] else 0
        )

        # Ø£Ø³Ø±Ø¹ Ù†Ù…ÙˆØ°Ø¬
        fastest_model = max(
            self.benchmark_results.items(),
            key=lambda x: x[1]["timing"]["fps"]["mean"] if "error" not in x[1] else 0
        )

        # Ø£Ù‚Ù„ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ù„Ù„Ù…ÙˆØ§Ø±Ø¯
        most_efficient_model = min(
            self.benchmark_results.items(),
            key=lambda x: x[1]["resource_usage"]["memory_used"]["mean"] if "error" not in x[1] else float('inf')
        )

        print(f"ğŸ† Ø£ÙØ¶Ù„ Ø¯Ù‚Ø©: {best_accuracy_model[0]} ({best_accuracy_model[1]['metrics']['accuracy']['mean']*100:.2f}%)")
        print(f"ğŸš€ Ø§Ù„Ø£Ø³Ø±Ø¹: {fastest_model[0]} ({fastest_model[1]['timing']['fps']['mean']:.2f} FPS)")
        print(f"ğŸ’¾ Ø§Ù„Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø©: {most_efficient_model[0]} ({most_efficient_model[1]['resource_usage']['memory_used']['mean']:.2f} GB)")

        # ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
        print("\nğŸ“ ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:")
        print("â€¢ Ù„Ù„Ø¯Ù‚Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ©: Ø§Ø³ØªØ®Ø¯Ù…", best_accuracy_model[0])
        print("â€¢ Ù„Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ÙÙˆØ±ÙŠØ©: Ø§Ø³ØªØ®Ø¯Ù…", fastest_model[0])
        print("â€¢ Ù„Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ù…Ø­Ø¯ÙˆØ¯Ø©: Ø§Ø³ØªØ®Ø¯Ù…", most_efficient_model[0])

    def create_visualizations(self) -> None:
        """
        Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø®Ø·Ø·Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©
        """

        if not self.benchmark_results:
            return

        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø±Ø³Ù…
        models = []
        accuracies = []
        fps_values = []
        memory_usage = []

        for model_name, results in self.benchmark_results.items():
            if "error" not in results:
                models.append(model_name)
                accuracies.append(results["metrics"]["accuracy"]["mean"] * 100)
                fps_values.append(results["timing"]["fps"]["mean"])
                memory_usage.append(results["resource_usage"]["memory_used"]["mean"])

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø®Ø·Ø·Ø§Øª
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Ù…Ù‚Ø§Ø±Ù†Ø© Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬', fontsize=16, fontweight='bold')

        # Ù…Ø®Ø·Ø· Ø§Ù„Ø¯Ù‚Ø©
        axes[0, 0].bar(models, accuracies, color='skyblue', alpha=0.7)
        axes[0, 0].set_title('Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ (%)')
        axes[0, 0].set_ylabel('Ø§Ù„Ø¯Ù‚Ø© (%)')
        axes[0, 0].tick_params(axis='x', rotation=45)

        # Ù…Ø®Ø·Ø· Ø§Ù„Ø³Ø±Ø¹Ø©
        axes[0, 1].bar(models, fps_values, color='lightgreen', alpha=0.7)
        axes[0, 1].set_title('Ø³Ø±Ø¹Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ (FPS)')
        axes[0, 1].set_ylabel('Ø¥Ø·Ø§Ø±Ø§Øª ÙÙŠ Ø§Ù„Ø«Ø§Ù†ÙŠØ©')
        axes[0, 1].tick_params(axis='x', rotation=45)

        # Ù…Ø®Ø·Ø· Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        axes[1, 0].bar(models, memory_usage, color='lightcoral', alpha=0.7)
        axes[1, 0].set_title('Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø© (GB)')
        axes[1, 0].set_ylabel('Ø§Ù„Ø°Ø§ÙƒØ±Ø© (GB)')
        axes[1, 0].tick_params(axis='x', rotation=45)

        # Ù…Ø®Ø·Ø· Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ø³Ø±Ø¹Ø©
        scatter = axes[1, 1].scatter(fps_values, accuracies, c=memory_usage,
                                     cmap='viridis', s=100, alpha=0.7)
        axes[1, 1].set_xlabel('Ø§Ù„Ø³Ø±Ø¹Ø© (FPS)')
        axes[1, 1].set_ylabel('Ø§Ù„Ø¯Ù‚Ø© (%)')
        axes[1, 1].set_title('Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ø³Ø±Ø¹Ø©')

        # Ø¥Ø¶Ø§ÙØ© ØªØ³Ù…ÙŠØ§Øª Ù„Ù„Ù†Ù‚Ø§Ø·
        for i, model in enumerate(models):
            axes[1, 1].annotate(model, (fps_values[i], accuracies[i]),
                                xytext=(5, 5), textcoords='offset points', fontsize=8)

        # Ø¥Ø¶Ø§ÙØ© Ø´Ø±ÙŠØ· Ø§Ù„Ø£Ù„ÙˆØ§Ù†
        cbar = plt.colorbar(scatter, ax=axes[1, 1])
        cbar.set_label('Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø© (GB)')

        plt.tight_layout()

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ù…Ù„Ù ÙØ±ÙŠØ¯ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ù„ÙŠ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = self.reports_dir / f"model_comparison_{timestamp}.png"

        plt.savefig(report_path, dpi=300, bbox_inches='tight')
        print(f"\nğŸ“Š ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ø®Ø·Ø·Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ© ÙÙŠ: {report_path}")

    def save_detailed_report(self) -> None:
        """
        Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„
        """

        report = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "benchmark_results": self.benchmark_results,
            "summary": self.generate_summary()
        }

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ù…Ù„Ù ÙØ±ÙŠØ¯ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ù„ÙŠ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = self.reports_dir / f"benchmark_report_{timestamp}.json"

        with open(report_path, "w", encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…ÙØµÙ„ ÙÙŠ: {report_path}")

    def generate_summary(self) -> Dict[str, Any]:
        """
        Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬

        Ø§Ù„Ø¹Ø§Ø¦Ø¯:
            Ù‚Ø§Ù…ÙˆØ³ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        """

        if not self.benchmark_results:
            return {}

        valid_results = {k: v for k, v in self.benchmark_results.items() if "error" not in v}

        if not valid_results:
            return {}

        summary = {
            "total_models_tested": len(self.benchmark_results),
            "successful_models": len(valid_results),
            "average_accuracy": np.mean([v["metrics"]["accuracy"]["mean"] for v in valid_results.values()]),
            "average_fps": np.mean([v["timing"]["fps"]["mean"] for v in valid_results.values()]),
            "average_memory_usage": np.mean([v["resource_usage"]["memory_used"]["mean"] for v in valid_results.values()])
        }

        return summary

    def get_gpu_memory(self) -> float:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø°Ø§ÙƒØ±Ø© GPU

        Ø§Ù„Ø¹Ø§Ø¦Ø¯:
            Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø°Ø§ÙƒØ±Ø© GPU Ø¨Ø§Ù„Ø¬ÙŠØ¬Ø§Ø¨Ø§ÙŠØª
        """

        if torch.cuda.is_available():
            return torch.cuda.memory_allocated() / (1024**3)  # GB
        return 0

    def get_best_model_for_scenario(self, scenario: str) -> str:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ Ù…Ø¹ÙŠÙ†

        Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª:
            scenario: Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ('accuracy', 'speed', 'efficiency')

        Ø§Ù„Ø¹Ø§Ø¦Ø¯:
            Ø§Ø³Ù… Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
        """

        if not self.benchmark_results:
            return "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù…ØªØ§Ø­Ø©"

        valid_results = {k: v for k, v in self.benchmark_results.items() if "error" not in v}

        if not valid_results:
            return "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ ØµØ­ÙŠØ­Ø©"

        if scenario == 'accuracy':
            best_model = max(
                valid_results.items(),
                key=lambda x: x[1]["metrics"]["accuracy"]["mean"]
            )
            return best_model[0]

        elif scenario == 'speed':
            best_model = max(
                valid_results.items(),
                key=lambda x: x[1]["timing"]["fps"]["mean"]
            )
            return best_model[0]

        elif scenario == 'efficiency':
            best_model = min(
                valid_results.items(),
                key=lambda x: x[1]["resource_usage"]["memory_used"]["mean"]
            )
            return best_model[0]

        else:
            return "Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"

    def export_results_to_csv(self, file_path: Optional[str] = None) -> str:
        """
        ØªØµØ¯ÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ Ù…Ù„Ù CSV

        Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª:
            file_path: Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ù„Ù„ØªØµØ¯ÙŠØ± (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)

        Ø§Ù„Ø¹Ø§Ø¦Ø¯:
            Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…ØµØ¯Ø±
        """

        if not self.benchmark_results:
            return "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù„Ù„ØªØµØ¯ÙŠØ±"

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØµØ¯ÙŠØ±
        data = []
        for model_name, results in self.benchmark_results.items():
            if "error" not in results:
                metrics = results["metrics"]
                timing = results["timing"]
                resources = results["resource_usage"]
                stability = results["stability"]

                row = {
                    "model_name": model_name,
                    "accuracy": metrics["accuracy"]["mean"],
                    "accuracy_std": metrics["accuracy"]["std"],
                    "precision": metrics["precision"]["mean"],
                    "recall": metrics["recall"]["mean"],
                    "f1_score": metrics["f1_score"]["mean"],
                    "avg_confidence": metrics["avg_confidence"]["mean"],
                    "fps": timing["fps"]["mean"],
                    "avg_inference_time": timing["avg_inference_time"]["mean"],
                    "memory_used": resources["memory_used"]["mean"],
                    "cpu_usage": resources["cpu_usage"]["mean"],
                    "consistency_score": stability["consistency_score"]
                }

                data.append(row)

        # Ø¥Ù†Ø´Ø§Ø¡ DataFrame
        df = pd.DataFrame(data)

        # ØªØ­Ø¯ÙŠØ¯ Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù
        if file_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            file_path = str(self.reports_dir / f"benchmark_results_{timestamp}.csv")

        # ØªØµØ¯ÙŠØ± Ø¥Ù„Ù‰ CSV
        df.to_csv(file_path, index=False)

        return file_path

    def analyze_model_learning(self, model_name: str) -> Dict[str, Any]:
        """
        ØªØ­Ù„ÙŠÙ„ ÙƒÙŠÙÙŠØ© ØªØ¹Ù„Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØªØ­Ø¯ÙŠØ¯ Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ© ÙˆØ§Ù„Ø¶Ø¹Ù

        Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª:
            model_name: Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ­Ù„ÙŠÙ„

        Ø§Ù„Ø¹Ø§Ø¦Ø¯:
            Ù‚Ø§Ù…ÙˆØ³ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„
        """

        if model_name not in self.benchmark_results:
            return {"error": f"Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name} ØºÙŠØ± Ù…ØªØ§Ø­ ÙÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"}

        results = self.benchmark_results[model_name]

        if "error" in results:
            return {"error": f"Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name} Ù„Ø¯ÙŠÙ‡ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"}

        # ØªØ­Ù„ÙŠÙ„ Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ© ÙˆØ§Ù„Ø¶Ø¹Ù
        metrics = results["metrics"]
        timing = results["timing"]

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„ØªÙŠ ÙŠØªÙÙˆÙ‚ ÙÙŠÙ‡Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        # (Ù‡Ø°Ø§ ÙŠØªØ·Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù…Ù† Ù…ØµÙÙˆÙØ© Ø§Ù„Ø®Ù„Ø·)

        analysis = {
            "model_name": model_name,
            "strengths": [],
            "weaknesses": [],
            "recommendations": []
        }

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù‚Ø©
        if metrics["accuracy"]["mean"] > 0.9:
            analysis["strengths"].append("Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹")
        elif metrics["accuracy"]["mean"] > 0.8:
            analysis["strengths"].append("Ø¯Ù‚Ø© Ø¬ÙŠØ¯Ø©")
        else:
            analysis["weaknesses"].append("Ø¯Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø©")
            analysis["recommendations"].append("ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø© Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø²ÙŠØ§Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø£Ùˆ ØªØ¹Ø¯ÙŠÙ„ Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø±Ø¹Ø©
        if timing["fps"]["mean"] > 30:
            analysis["strengths"].append("Ø³Ø±Ø¹Ø© Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹")
        elif timing["fps"]["mean"] > 10:
            analysis["strengths"].append("Ø³Ø±Ø¹Ø© Ø¬ÙŠØ¯Ø©")
        else:
            analysis["weaknesses"].append("Ø³Ø±Ø¹Ø© Ù…Ù†Ø®ÙØ¶Ø©")
            analysis["recommendations"].append("ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø³Ø±Ø¹Ø© Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ³Ø±ÙŠØ¹")

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±
        if results["stability"]["consistency_score"] > 0.95:
            analysis["strengths"].append("Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ù…Ù…ØªØ§Ø²")
        elif results["stability"]["consistency_score"] > 0.9:
            analysis["strengths"].append("Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø¬ÙŠØ¯")
        else:
            analysis["weaknesses"].append("Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ù…Ù†Ø®ÙØ¶")
            analysis["recommendations"].append("ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø²ÙŠØ§Ø¯Ø© ØªÙ†ÙˆØ¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨")

        return analysis


class ResourceMonitor:
    """Ù…Ø±Ø§Ù‚Ø¨ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""

    def __init__(self):
        """
        ØªÙ‡ÙŠØ¦Ø© Ù…Ø±Ø§Ù‚Ø¨ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
        """
        self.monitoring = False
        self.cpu_usage = []
        self.memory_usage = []
        self.monitor_thread = None

    def start_monitoring(self) -> None:
        """
        Ø¨Ø¯Ø¡ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
        """
        self.monitoring = True
        self.cpu_usage = []
        self.memory_usage = []
        self.monitor_thread = threading.Thread(target=self._monitor_resources)
        self.monitor_thread.start()

    def stop_monitoring(self) -> Dict[str, float]:
        """
        Ø¥ÙŠÙ‚Ø§Ù Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…ÙˆØ§Ø±Ø¯

        Ø§Ù„Ø¹Ø§Ø¦Ø¯:
            Ù‚Ø§Ù…ÙˆØ³ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
        """
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()

        return {
            "avg_cpu_usage": np.mean(self.cpu_usage) if self.cpu_usage else 0,
            "peak_cpu_usage": np.max(self.cpu_usage) if self.cpu_usage else 0,
            "avg_memory": np.mean(self.memory_usage) if self.memory_usage else 0,
            "peak_memory": np.max(self.memory_usage) if self.memory_usage else 0
        }

    def _monitor_resources(self) -> None:
        """
        Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ ÙÙŠ Ø®ÙŠØ· Ù…Ù†ÙØµÙ„
        """

        while self.monitoring:
            try:
                cpu_percent = psutil.cpu_percent(interval=0.1)
                memory_percent = psutil.virtual_memory().percent

                self.cpu_usage.append(cpu_percent)
                self.memory_usage.append(memory_percent)

                time.sleep(0.1)  # Ù…Ø±Ø§Ù‚Ø¨Ø© ÙƒÙ„ 100ms
            except BaseException:
                break


# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ø¸Ø§Ù… Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!")
