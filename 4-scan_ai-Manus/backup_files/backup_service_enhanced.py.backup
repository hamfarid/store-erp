"""

خدمة النسخ الاحتياطي والاستعادة المحسنة
توفر هذه الوحدة خدمات متقدمة لإنشاء وإدارة النسخ الاحتياطية واستعادة البيانات
مع دعم قواعد البيانات المتعددة ومعلومات التعلم وبيانات الحاويات
"""

import os
import json
import time
import shutil
import tarfile
import datetime
import logging
import subprocess
import threading
import queue

from pathlib import Path


# إعداد التسجيل
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("backup_service.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class BackupServiceEnhanced:
    """خدمة النسخ الاحتياطي والاستعادة المحسنة"""

    def __init__(self, config=None):
        """
        تهيئة خدمة النسخ الاحتياطي المحسنة

        Args:
            config (dict, optional): إعدادات النسخ الاحتياطي. Defaults to None.
        """
        # الإعدادات الافتراضية
        self.default_config = {
            'backup_dir': os.path.join(os.path.dirname(os.path.abspath(__file__)), 'backups""temp_dir': os.path.join(os.path.dirname(os.path.abspath(__file__)), 'temp""max_backups': 10,
            'compression': 'gz',  # 'gz', 'bz2', 'xz'
            'exclude_patterns': [
                '.env""*.pyc""__pycache__""*.log""node_modules"".git"".cache""*.tmp'
            ],
            'include_txt_md': True,  # تضمين جميع ملفات .txt و .md
            'backup_db': True,
            'db_configs""main""host': 'db""port""': 'agri_ai_user""password': 'agri_ai_password""database': 'agri_ai_db'
                },
                'test""host': 'db_test""port""': 'agri_ai_user""password': 'agri_ai_password""database': 'agri_ai_test_db'
                },
                'setup""host': 'db_setup""port""': 'agri_ai_user""password': 'agri_ai_password""database': 'agri_ai_setup_db'
                }
            },
            'data_dirs""config': '/app/config""models': '/app/models""media': '/app/media""uploads': '/app/uploads""data': '/app/data""static': '/app/static'
            },
            'docker""use_volumesTRUEuse_bind_mountsTRUEvolumes': [
                    'postgres_data""postgres_test_data""postgres_setup_data""redis_data""rabbitmq_data""elasticsearch_data""etcd_data""minio_data'
                ],
                'bind_mounts': [
                    '/app/data""/app/logs""/app/media""/app/static""/app/uploads""/app/models""/app/backups""/app/config'
                ]
            },
            'schedule""enabledTRUEfrequency': 'daily',  # 'hourly', 'daily', 'weekly', 'monthly'
                'time': '02:00""day_of_week': 1,  # 0 = Monday, 6 = Sunday (for weekly)
                'day_of_month': 1  # 1-31 (for monthly)
            }
        }

        # دمج الإعدادات المخصصة مع الإعدادات الافتراضية
        self.config = self.default_config.copy()
        if config:
            self._deep_update(self.config, config)

        # التأكد من وجود مجلدات النسخ الاحتياطي والمؤقت
        os.makedirs(self.config['backup_dir'], exist_ok=True)
        os.makedirs(self.config['temp_dir'], exist_ok=True)

        # قائمة العمليات الجارية
        self.running_tasks = {}
        self.task_queue = queue.Queue()
        self.worker_thread = None

        # بدء خيط العمل
        self._start_worker()

    def _deep_update(self, d, u):
        """
        تحديث عميق للقواميس المتداخلة

        Args:
            d (dict): القاموس الأصلي
            u (dict): القاموس الجديد للتحديث

        Returns:
            dict: القاموس المحدث
        """
        for k, v in u.items():
            if isinstance(v, dict) and k in d and isinstance(d[k], dict):
                self._deep_update(d[k], v)
            else:
                d[k] = v
        return d

    def _start_worker(self):
        """بدء خيط العمل لمعالجة المهام في الخلفية"""
        if self.worker_thread is None or not self.worker_thread.is_alive():
            self.worker_thread = threading.Thread(target=self._process_tasks)
            self.worker_thread.daemon = True
            self.worker_thread.start()

    def _process_tasks(self):
        """معالجة المهام في الخلفية"""
        while True:
            try:
                task_id, task_type, args, kwargs = self.task_queue.get()

                # تحديث حالة المهمة
                self.running_tasks[task_id]['status'] = 'running'

                try:
                    # تنفيذ المهمة المناسبة
                    if task_type == 'create_backup':
                        result = self._create_backup_internal(*args, **kwargs)
                    elif task_type == 'restore_backup':
                        result = self._restore_backup_internal(*args, **kwargs)
                    elif task_type == 'export_data':
                        result = self._export_data_internal(*args, **kwargs)
                    elif task_type == 'import_data':
                        result = self._import_data_internal(*args, **kwargs)
                    else:
                        result = {'success': False, 'error': f'Unknown task type: {task_type}'}

                    # تحديث حالة المهمة بالنتيجة
                    self.running_tasks[task_id].update({
                        'status': 'completed',
                        'completed_at': datetime.datetime.now().isoformat(),
                        'result': result
                    })
                except Exception as e:
                    logger.exception(f"Error processing task {task_id}: {str(e)}")
                    # تحديث حالة المهمة بالخطأ
                    self.running_tasks[task_id].update({
                        'status': 'failed',
                        'completed_at': datetime.datetime.now().isoformat(),
                        'error': str(e)
                    })

                # إشارة إلى اكتمال المهمة
                self.task_queue.task_done()
            except Exception as e:
                logger.exception(f"Worker thread error: {str(e)}")
                time.sleep(1)  # تجنب استهلاك CPU في حالة الخطأ

    def create_backup(self, name=None, description=None, include_dirs=None, exclude_patterns=None,
                      backup_dbs=None, db_names=None, include_volumes=None, include_bind_mounts=None,
                      include_txt_md=None):
        """
        إنشاء نسخة احتياطية جديدة

        Args:
            name (str, optional): اسم النسخة الاحتياطية. Defaults to None.
            description (str, optional): وصف النسخة الاحتياطية. Defaults to None.
            include_dirs (list, optional): قائمة المجلدات المراد نسخها. Defaults to None.
            exclude_patterns (list, optional): قائمة أنماط الملفات المراد استبعادها. Defaults to None.
            backup_dbs (bool, optional): ما إذا كان يجب نسخ قواعد البيانات. Defaults to None.
            db_names (list, optional): قائمة أسماء قواعد البيانات المراد نسخها. Defaults to None.
            include_volumes (bool, optional): ما إذا كان يجب تضمين Docker Volumes. Defaults to None.
            include_bind_mounts (bool, optional): ما إذا كان يجب تضمين Bind Mounts. Defaults to None.
            include_txt_md (bool, optional): ما إذا كان يجب تضمين جميع ملفات .txt و .md. Defaults to None.

        Returns:
            dict: معلومات المهمة
        """
        # إنشاء معرف فريد للمهمة
        task_id = f"backup_{int(time.time())}_{os.urandom(4).hex()}"

        # إنشاء معلومات المهمة
        task_info = {
            'idTASK_IDtype': 'backup""name': name or f"backup_{datetime.datetime.now().strftime(YMD_HMS)}",
            'description': description,
            'status': 'pending""created_atDATETIMEDATETIMENOWISOFORMATcompleted_atNONEresultNONEerrorNONE_إضافة_المهمةcreate_backup""name': name,
                'description': description,
                'include_dirsINCLUDE_DIRSexclude_patterns': exclude_patterns,
                'backup_dbs': backup_dbs,
                'db_namesDB_NAMESinclude_volumesINCLUDE_VOLUMESinclude_bind_mounts': include_bind_mounts,
                'include_txt_md': include_txt_md
            }
        ))

        return task_info

    def _create_backup_internal(self, name=None, description=None, include_dirs=None, exclude_patterns=None,
                                backup_dbs=None, db_names=None, include_volumes=None, include_bind_mounts=None,
                                include_txt_md=None):
        """
        التنفيذ الداخلي لإنشاء نسخة احتياطية

        Args:
            name (str, optional): اسم النسخة الاحتياطية. Defaults to None.
            description (str, optional): وصف النسخة الاحتياطية. Defaults to None.
            include_dirs (list, optional): قائمة المجلدات المراد نسخها. Defaults to None.
            exclude_patterns (list, optional): قائمة أنماط الملفات المراد استبعادها. Defaults to None.
            backup_dbs (bool, optional): ما إذا كان يجب نسخ قواعد البيانات. Defaults to None.
            db_names (list, optional): قائمة أسماء قواعد البيانات المراد نسخها. Defaults to None.
            include_volumes (bool, optional): ما إذا كان يجب تضمين Docker Volumes. Defaults to None.
            include_bind_mounts (bool, optional): ما إذا كان يجب تضمين Bind Mounts. Defaults to None.
            include_txt_md (bool, optional): ما إذا كان يجب تضمين جميع ملفات .txt و .md. Defaults to None.

        Returns:
            dict: نتيجة العملية
        """
        try:
            # استخدام القيم الافتراضية إذا لم يتم تحديدها
            backup_name = name or f"backup_{datetime.datetime.now().strftime(YMD_HMS)}"
            backup_description = description or f"Automatic backup created on {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            dirs_to_include = include_dirs or list(self.config['data_dirs'].values())
            patterns_to_exclude = exclude_patterns or self.config['exclude_patterns']
            should_backup_dbs = backup_dbs if backup_dbs is not None else self.config['backup_db']
            db_list = db_names or list(self.config['db_configs'].keys())
            should_include_volumes = include_volumes if include_volumes is not None else self.config['docker']['use_volumes']
            should_include_bind_mounts = include_bind_mounts if include_bind_mounts is not None else self.config['docker']['use_bind_mounts']
            should_include_txt_md = include_txt_md if include_txt_md is not None else self.config['include_txt_md']

            # إنشاء مجلد مؤقت للنسخة الاحتياطية
            backup_temp_dir = os.path.join(self.config['temp_dir'], backup_name)
            os.makedirs(backup_temp_dir, exist_ok=True)

            # إنشاء مجلد للبيانات
            data_dir = os.path.join(backup_temp_dir, 'data')
            os.makedirs(data_dir, exist_ok=True)

            # نسخ المجلدات المحددة (Bind Mounts)
            if should_include_bind_mounts:
                for dir_path in dirs_to_include:
                    if os.path.exists(dir_path):
                        dest_dir = os.path.join(data_dir, os.path.basename(dir_path))
                        logger.info(f"Copying directory {dir_path} to {dest_dir}")
                        self._copy_directory(dir_path, dest_dir, patterns_to_exclude, include_txt_md=should_include_txt_md)
                    else:
                        logger.warning(f"Directory {dir_path} does not exist, skipping")

            # نسخ قواعد البيانات إذا كان مطلوباً
            db_backup_paths = {}
            if should_backup_dbs:
                db_dir = os.path.join(backup_temp_dir, 'databases')
                os.makedirs(db_dir, exist_ok=True)

                for db_name in db_list:
                    if db_name in self.config['db_configs']:
                        db_config = self.config['db_configs'][db_name]
                        db_backup_path = os.path.join(db_dir, f"{db_name}.sql")
                        logger.info(f"Backing up database {db_name} to {db_backup_path}")
                        self._backup_database(db_backup_path, db_config)
                        db_backup_paths[db_name] = os.path.relpath(db_backup_path, backup_temp_dir)

            # نسخ Docker Volumes إذا كان مطلوباً
            volume_backup_paths = {}
            if should_include_volumes:
                volumes_dir = os.path.join(backup_temp_dir, 'volumes')
                os.makedirs(volumes_dir, exist_ok=True)

                for volume_name in self.config['docker']['volumes']:
                    volume_backup_path = os.path.join(volumes_dir, f"{volume_name}.tar")
                    logger.info(f"Backing up Docker volume {volume_name} to {volume_backup_path}")
                    success = self._backup_docker_volume(volume_name, volume_backup_path)
                    if success:
                        volume_backup_paths[volume_name] = os.path.relpath(volume_backup_path, backup_temp_dir)

            # إنشاء ملف التكوين للنسخة الاحتياطية
            backup_config = {
                'nameBACKUP_NAMEdescription': backup_description,
                'created_at': datetime.datetime.now().isoformat(),
                'include_dirs': dirs_to_include,
                'exclude_patterns': patterns_to_exclude,
                'backup_dbs': should_backup_dbs,
                'db_backup_paths': db_backup_paths,
                'include_volumes': should_include_volumes,
                'volume_backup_paths': volume_backup_paths,
                'include_bind_mounts': should_include_bind_mounts,
                'include_txt_md': should_include_txt_md
            }

            # حفظ ملف التكوين
            config_path = os.path.join(backup_temp_dir, 'backup_config.json')
            with open(config_path, 'wENCODINGutf-8') as f:
                json.dump(backup_config, f, ensure_ascii=False, indent=2)

            # إنشاء ملف النسخة الاحتياطية المضغوط
            backup_file_path = os.path.join(self.config['backup_dir'], f"{backup_name}.tar.{self.config['compression']}")
            logger.info(f"Creating compressed backup file {backup_file_path}")

            # تحديد طريقة الضغط
            compression_mode = f"w:{self.config['compression']}"
            with tarfile.open(backup_file_path, compression_mode) as tar:
                tar.add(backup_temp_dir, arcname=os.path.basename(backup_temp_dir))

            # حذف المجلد المؤقت
            shutil.rmtree(backup_temp_dir)

            # التحقق من عدد النسخ الاحتياطية وحذف الأقدم إذا تجاوز الحد
            self._cleanup_old_backups()

            # إنشاء معلومات النسخة الاحتياطية
            backup_info = {
                'nameBACKUP_NAMEdescription': backup_description,
                'created_at': backup_config['created_at'],
                'file_path': backup_file_path,
                'file_size': os.path.getsize(backup_file_path),
                'include_dirs': dirs_to_include,
                'backup_dbs': should_backup_dbs,
                'db_names': list(db_backup_paths.keys()),
                'include_volumes': should_include_volumes,
                'volume_names': list(volume_backup_paths.keys()),
                'include_bind_mounts': should_include_bind_mounts
            }

            logger.info(f"Backup {backup_name} created successfully")
            return {
                'successTRUEbackup_info': backup_info
            }
        except Exception as e:
            logger.exception(f"Error creating backup: {str(e)}")
            return {
                'successFALSEerror': str(e)
            }

    def restore_backup(self, backup_name=None, backup_file=None, restore_dbs=True,
                       db_names=None, restore_volumes=True, volume_names=None,
                       restore_bind_mounts=True, dir_names=None):
        """
        استعادة نسخة احتياطية

        Args:
            backup_name (str, optional): اسم النسخة الاحتياطية. Defaults to None.
            backup_file (str, optional): مسار ملف النسخة الاحتياطية. Defaults to None.
            restore_dbs (bool, optional): ما إذا كان يجب استعادة قواعد البيانات. Defaults to True.
            db_names (list, optional): قائمة أسماء قواعد البيانات المراد استعادتها. Defaults to None.
            restore_volumes (bool, optional): ما إذا كان يجب استعادة Docker Volumes. Defaults to True.
            volume_names (list, optional): قائمة أسماء الـ Volumes المراد استعادتها. Defaults to None.
            restore_bind_mounts (bool, optional): ما إذا كان يجب استعادة Bind Mounts. Defaults to True.
            dir_names (list, optional): قائمة أسماء المجلدات المراد استعادتها. Defaults to None.

        Returns:
            dict: معلومات المهمة
        """
        # التحقق من توفر المعلومات المطلوبة
        if not backup_name and not backup_file:
            raise ValueError("Either backup_name or backup_file must be provided")

        # إذا تم تحديد الاسم فقط، ابحث عن الملف المناسب
        if backup_name and not backup_file:
            for ext in ['gz', 'bz2', 'xz']:
                potential_file = os.path.join(self.config['backup_dir'], f"{backup_name}.tar.{ext}")
                if os.path.exists(potential_file):
                    backup_file = potential_file
                    break

            if not backup_file:
                raise FileNotFoundError(f"Backup file for {backup_name} not found")

        # إنشاء معرف فريد للمهمة
        task_id = f"restore_{int(time.time())}_{os.urandom(4).hex()}"

        # إنشاء معلومات المهمة
        task_info = {
            'idTASK_IDtype': 'restore""backup_name': backup_name,
            'backup_file': backup_file,
            'status': 'pending""created_atDATETIMEDATETIMENOWISOFORMATcompleted_atNONEresultNONEerrorNONE_إضافة_المهمةrestore_backup""backup_nameBACKUP_NAMEbackup_file': backup_file,
                'restore_dbs': restore_dbs,
                'db_namesDB_NAMESrestore_volumes': restore_volumes,
                'volume_namesVOLUME_NAMESrestore_bind_mounts': restore_bind_mounts,
                'dir_names': dir_names
            }
        ))

        return task_info

    def _restore_backup_internal(self, backup_name=None, backup_file=None, restore_dbs=True,
                                 db_names=None, restore_volumes=True, volume_names=None,
                                 restore_bind_mounts=True, dir_names=None):
        """
        التنفيذ الداخلي لاستعادة نسخة احتياطية

        Args:
            backup_name (str, optional): اسم النسخة الاحتياطية. Defaults to None.
            backup_file (str, optional): مسار ملف النسخة الاحتياطية. Defaults to None.
            restore_dbs (bool, optional): ما إذا كان يجب استعادة قواعد البيانات. Defaults to True.
            db_names (list, optional): قائمة أسماء قواعد البيانات المراد استعادتها. Defaults to None.
            restore_volumes (bool, optional): ما إذا كان يجب استعادة Docker Volumes. Defaults to True.
            volume_names (list, optional): قائمة أسماء الـ Volumes المراد استعادتها. Defaults to None.
            restore_bind_mounts (bool, optional): ما إذا كان يجب استعادة Bind Mounts. Defaults to True.
            dir_names (list, optional): قائمة أسماء المجلدات المراد استعادتها. Defaults to None.

        Returns:
            dict: نتيجة العملية
        """
        try:
            # التحقق من وجود ملف النسخة الاحتياطية
            if not os.path.exists(backup_file):
                raise FileNotFoundError(f"Backup file {backup_file} not found")

            # إنشاء مجلد مؤقت لاستخراج النسخة الاحتياطية
            extract_dir = os.path.join(self.config['temp_dir'], f"restore_{int(time.time())}")
            os.makedirs(extract_dir, exist_ok=True)

            # استخراج النسخة الاحتياطية
            logger.info(f"Extracting backup file {backup_file} to {extract_dir}")
            with tarfile.open(backup_file, 'r:*') as tar:
                tar.extractall(path=extract_dir)

            # البحث عن مجلد النسخة الاحتياطية داخل المجلد المستخرج
            backup_dirs = [d for d in os.listdir(extract_dir) if os.path.isdir(os.path.join(extract_dir, d))]
            if not backup_dirs:
                raise ValueError("No backup directory found in the extracted files")

            backup_dir = os.path.join(extract_dir, backup_dirs[0])

            # قراءة ملف التكوين
            config_path = os.path.join(backup_dir, 'backup_config.json')
            if not os.path.exists(config_path):
                raise FileNotFoundError(f"Backup configuration file not found in {backup_dir}")

            with open(config_path, 'rENCODINGutf-8') as f:
                backup_config = json.load(f)

            # استعادة المجلدات (Bind Mounts)
            if restore_bind_mounts and backup_config.get('include_bind_mounts', False):
                data_dir = os.path.join(backup_dir, 'data')
                if os.path.exists(data_dir):
                    # تحديد المجلدات المراد استعادتها
                    dirs_to_restore = dir_names if dir_names else [os.path.basename(d) for d in backup_config.get('include_dirs', [])]

                    for dir_name in dirs_to_restore:
                        src_dir = os.path.join(data_dir, dir_name)
                        if os.path.exists(src_dir):
                            # البحث عن المجلد المقابل في الإعدادات
                            target_dir = None
                            for path in self.config['data_dirs'].values():
                                if os.path.basename(path) == dir_name:
                                    target_dir = path
                                    break

                            if target_dir:
                                logger.info(f"Restoring directory {src_dir} to {target_dir}")

                                # إنشاء نسخة احتياطية من المجلد الحالي إذا كان موجوداً
                                if os.path.exists(target_dir):
                                    backup_target_dir = f"{target_dir}_backup_{int(time.time())}"
                                    logger.info(f"Creating backup of existing directory {target_dir} to {backup_target_dir}")
                                    shutil.move(target_dir, backup_target_dir)

                                # إنشاء المجلد الهدف إذا لم يكن موجوداً
                                os.makedirs(os.path.dirname(target_dir), exist_ok=True)

                                # نسخ المجلد من النسخة الاحتياطية
                                shutil.copytree(src_dir, target_dir)

            # استعادة قواعد البيانات
            if restore_dbs and backup_config.get('backup_dbs', False):
                db_backup_paths = backup_config.get('db_backup_paths', {})

                # تحديد قواعد البيانات المراد استعادتها
                dbs_to_restore = db_names if db_names else list(db_backup_paths.keys())

                for db_name in dbs_to_restore:
                    if db_name in db_backup_paths and db_name in self.config['db_configs']:
                        db_backup_path = os.path.join(backup_dir, db_backup_paths[db_name])
                        if os.path.exists(db_backup_path):
                            logger.info(f"Restoring database {db_name} from {db_backup_path}")
                            self._restore_database(db_backup_path, self.config['db_configs'][db_name])

            # استعادة Docker Volumes
            if restore_volumes and backup_config.get('include_volumes', False):
                volume_backup_paths = backup_config.get('volume_backup_paths', {})

                # تحديد الـ Volumes المراد استعادتها
                volumes_to_restore = volume_names if volume_names else list(volume_backup_paths.keys())

                for volume_name in volumes_to_restore:
                    if volume_name in volume_backup_paths:
                        volume_backup_path = os.path.join(backup_dir, volume_backup_paths[volume_name])
                        if os.path.exists(volume_backup_path):
                            logger.info(f"Restoring Docker volume {volume_name} from {volume_backup_path}")
                            self._restore_docker_volume(volume_name, volume_backup_path)

            # حذف المجلد المؤقت
            shutil.rmtree(extract_dir)

            logger.info(f"Backup {backup_name or os.path.basename(backup_file)} restored successfully")
            return {
                'successTRUEmessage': f"Backup {backup_name or os.path.basename(backup_file)} restored successfully"
            }
        except Exception as e:
            logger.exception(f"Error restoring backup: {str(e)}")
            return {
                'successFALSEerror': str(e)
            }

    def export_data(self, target_dir, include_dbs=True, db_names=None, include_volumes=False,
                    volume_names=None, include_dirs=True, dir_names=None):
        """
        تصدير البيانات إلى مجلد خارجي

        Args:
            target_dir (str): المجلد الهدف للتصدير
            include_dbs (bool, optional): ما إذا كان يجب تصدير قواعد البيانات. Defaults to True.
            db_names (list, optional): قائمة أسماء قواعد البيانات المراد تصديرها. Defaults to None.
            include_volumes (bool, optional): ما إذا كان يجب تصدير Docker Volumes. Defaults to False.
            volume_names (list, optional): قائمة أسماء الـ Volumes المراد تصديرها. Defaults to None.
            include_dirs (bool, optional): ما إذا كان يجب تصدير المجلدات. Defaults to True.
            dir_names (list, optional): قائمة أسماء المجلدات المراد تصديرها. Defaults to None.

        Returns:
            dict: معلومات المهمة
        """
        # التحقق من وجود المجلد الهدف
        if not os.path.exists(target_dir):
            os.makedirs(target_dir, exist_ok=True)

        # إنشاء معرف فريد للمهمة
        task_id = f"export_{int(time.time())}_{os.urandom(4).hex()}"

        # إنشاء معلومات المهمة
        task_info = {
            'idTASK_IDtype': 'export""target_dir': target_dir,
            'status': 'pending""created_atDATETIMEDATETIMENOWISOFORMATcompleted_atNONEresultNONEerrorNONE_إضافة_المهمةexport_data""target_dir': target_dir,
                'include_dbs': include_dbs,
                'db_namesDB_NAMESinclude_volumesINCLUDE_VOLUMESvolume_namesVOLUME_NAMESinclude_dirsINCLUDE_DIRSdir_names': dir_names
            }
        ))

        return task_info

    def _export_data_internal(self, target_dir, include_dbs=True, db_names=None, include_volumes=False,
                              volume_names=None, include_dirs=True, dir_names=None):
        """
        التنفيذ الداخلي لتصدير البيانات إلى مجلد خارجي

        Args:
            target_dir (str): المجلد الهدف للتصدير
            include_dbs (bool, optional): ما إذا كان يجب تصدير قواعد البيانات. Defaults to True.
            db_names (list, optional): قائمة أسماء قواعد البيانات المراد تصديرها. Defaults to None.
            include_volumes (bool, optional): ما إذا كان يجب تصدير Docker Volumes. Defaults to False.
            volume_names (list, optional): قائمة أسماء الـ Volumes المراد تصديرها. Defaults to None.
            include_dirs (bool, optional): ما إذا كان يجب تصدير المجلدات. Defaults to True.
            dir_names (list, optional): قائمة أسماء المجلدات المراد تصديرها. Defaults to None.

        Returns:
            dict: نتيجة العملية
        """
        try:
            # إنشاء مجلد للتصدير
            export_dir = os.path.join(target_dir, f"export_{datetime.datetime.now().strftime(YMD_HMS)}")
            os.makedirs(export_dir, exist_ok=True)

            exported_items = {
                'databases""volumes""directories': []
            }

            # تصدير قواعد البيانات
            if include_dbs:
                db_export_dir = os.path.join(export_dir, 'databases')
                os.makedirs(db_export_dir, exist_ok=True)

                # تحديد قواعد البيانات المراد تصديرها
                dbs_to_export = db_names if db_names else list(self.config['db_configs'].keys())

                for db_name in dbs_to_export:
                    if db_name in self.config['db_configs']:
                        db_config = self.config['db_configs'][db_name]
                        db_export_path = os.path.join(db_export_dir, f"{db_name}.sql")
                        logger.info(f"Exporting database {db_name} to {db_export_path}")
                        self._backup_database(db_export_path, db_config)
                        exported_items['databases'].append(db_name)

            # تصدير Docker Volumes
            if include_volumes:
                volume_export_dir = os.path.join(export_dir, 'volumes')
                os.makedirs(volume_export_dir, exist_ok=True)

                # تحديد الـ Volumes المراد تصديرها
                volumes_to_export = volume_names if volume_names else self.config['docker']['volumes']

                for volume_name in volumes_to_export:
                    volume_export_path = os.path.join(volume_export_dir, f"{volume_name}.tar")
                    logger.info(f"Exporting Docker volume {volume_name} to {volume_export_path}")
                    success = self._backup_docker_volume(volume_name, volume_export_path)
                    if success:
                        exported_items['volumes'].append(volume_name)

            # تصدير المجلدات
            if include_dirs:
                dir_export_dir = os.path.join(export_dir, 'directories')
                os.makedirs(dir_export_dir, exist_ok=True)

                # تحديد المجلدات المراد تصديرها
                dirs_to_export = dir_names if dir_names else list(self.config['data_dirs'].keys())

                for dir_name in dirs_to_export:
                    if dir_name in self.config['data_dirs']:
                        dir_path = self.config['data_dirs'][dir_name]
                        if os.path.exists(dir_path):
                            dir_export_path = os.path.join(dir_export_dir, dir_name)
                            logger.info(f"Exporting directory {dir_path} to {dir_export_path}")
                            shutil.copytree(dir_path, dir_export_path)
                            exported_items['directories'].append(dir_name)

            # إنشاء ملف التكوين للتصدير
            export_config = {
                'exported_at': datetime.datetime.now().isoformat(),
                'exported_items': exported_items
            }

            # حفظ ملف التكوين
            config_path = os.path.join(export_dir, 'export_config.json')
            with open(config_path, 'wENCODINGutf-8') as f:
                json.dump(export_config, f, ensure_ascii=False, indent=2)

            logger.info(f"Data exported successfully to {export_dir}")
            return {
                'successTRUEexport_dir': export_dir,
                'exported_items': exported_items
            }
        except Exception as e:
            logger.exception(f"Error exporting data: {str(e)}")
            return {
                'successFALSEerror': str(e)
            }

    def import_data(self, source_dir, include_dbs=True, db_names=None, include_volumes=False,
                    volume_names=None, include_dirs=True, dir_names=None):
        """
        استيراد البيانات من مجلد خارجي

        Args:
            source_dir (str): المجلد المصدر للاستيراد
            include_dbs (bool, optional): ما إذا كان يجب استيراد قواعد البيانات. Defaults to True.
            db_names (list, optional): قائمة أسماء قواعد البيانات المراد استيرادها. Defaults to None.
            include_volumes (bool, optional): ما إذا كان يجب استيراد Docker Volumes. Defaults to False.
            volume_names (list, optional): قائمة أسماء الـ Volumes المراد استيرادها. Defaults to None.
            include_dirs (bool, optional): ما إذا كان يجب استيراد المجلدات. Defaults to True.
            dir_names (list, optional): قائمة أسماء المجلدات المراد استيرادها. Defaults to None.

        Returns:
            dict: معلومات المهمة
        """
        # التحقق من وجود المجلد المصدر
        if not os.path.exists(source_dir):
            raise FileNotFoundError(f"Source directory {source_dir} not found")

        # إنشاء معرف فريد للمهمة
        task_id = f"import_{int(time.time())}_{os.urandom(4).hex()}"

        # إنشاء معلومات المهمة
        task_info = {
            'idTASK_IDtype': 'import""source_dir': source_dir,
            'status': 'pending""created_atDATETIMEDATETIMENOWISOFORMATcompleted_atNONEresultNONEerrorNONE_إضافة_المهمةimport_data""source_dir': source_dir,
                'include_dbs': include_dbs,
                'db_namesDB_NAMESinclude_volumesINCLUDE_VOLUMESvolume_namesVOLUME_NAMESinclude_dirsINCLUDE_DIRSdir_names': dir_names
            }
        ))

        return task_info

    def _import_data_internal(self, source_dir, include_dbs=True, db_names=None, include_volumes=False,
                              volume_names=None, include_dirs=True, dir_names=None):
        """
        التنفيذ الداخلي لاستيراد البيانات من مجلد خارجي

        Args:
            source_dir (str): المجلد المصدر للاستيراد
            include_dbs (bool, optional): ما إذا كان يجب استيراد قواعد البيانات. Defaults to True.
            db_names (list, optional): قائمة أسماء قواعد البيانات المراد استيرادها. Defaults to None.
            include_volumes (bool, optional): ما إذا كان يجب استيراد Docker Volumes. Defaults to False.
            volume_names (list, optional): قائمة أسماء الـ Volumes المراد استيرادها. Defaults to None.
            include_dirs (bool, optional): ما إذا كان يجب استيراد المجلدات. Defaults to True.
            dir_names (list, optional): قائمة أسماء المجلدات المراد استيرادها. Defaults to None.

        Returns:
            dict: نتيجة العملية
        """
        try:
            # التحقق من وجود ملف التكوين
            config_path = os.path.join(source_dir, 'export_config.json')
            if os.path.exists(config_path):
                with open(config_path, 'rENCODINGutf-8') as f:
                    _ = json.load(f)  # تحميل التكوين للتحقق من صحة الملف

                imported_items = {
                    'databases': [],
                    'volumes': [],
                    'directories': []
                }

                # استيراد قواعد البيانات
                if include_dbs:
                    db_import_dir = os.path.join(source_dir, 'databases')
                    if os.path.exists(db_import_dir):
                        # تحديد قواعد البيانات المراد استيرادها
                        available_dbs = [os.path.splitext(f)[0] for f in os.listdir(db_import_dir) if f.endswith('.sql')]
                        dbs_to_import = db_names if db_names else available_dbs

                        for db_name in dbs_to_import:
                            if db_name in self.config['db_configs'] and db_name in available_dbs:
                                db_import_path = os.path.join(db_import_dir, f"{db_name}.sql")
                                if os.path.exists(db_import_path):
                                    logger.info(f"Importing database {db_name} from {db_import_path}")
                                    self._restore_database(db_import_path, self.config['db_configs'][db_name])
                                    imported_items['databases'].append(db_name)

                # استيراد Docker Volumes
                if include_volumes:
                    volume_import_dir = os.path.join(source_dir, 'volumes')
                    if os.path.exists(volume_import_dir):
                        # تحديد الـ Volumes المراد استيرادها
                        available_volumes = [os.path.splitext(f)[0] for f in os.listdir(volume_import_dir) if f.endswith('.tar')]
                        volumes_to_import = volume_names if volume_names else available_volumes

                        for volume_name in volumes_to_import:
                            volume_import_path = os.path.join(volume_import_dir, f"{volume_name}.tar")
                            if os.path.exists(volume_import_path):
                                logger.info(f"Importing Docker volume {volume_name} from {volume_import_path}")
                                self._restore_docker_volume(volume_name, volume_import_path)
                                imported_items['volumes'].append(volume_name)

                # استيراد المجلدات
                if include_dirs:
                    dir_import_dir = os.path.join(source_dir, 'directories')
                    if os.path.exists(dir_import_dir):
                        # تحديد المجلدات المراد استيرادها
                        available_dirs = [d for d in os.listdir(dir_import_dir) if os.path.isdir(os.path.join(dir_import_dir, d))]
                        dirs_to_import = dir_names if dir_names else available_dirs

                        for dir_name in dirs_to_import:
                            if dir_name in self.config['data_dirs'] and dir_name in available_dirs:
                                dir_import_path = os.path.join(dir_import_dir, dir_name)
                                target_dir = self.config['data_dirs'][dir_name]

                                if os.path.exists(dir_import_path):
                                    logger.info(f"Importing directory {dir_name} from {dir_import_path} to {target_dir}")

                                    # إنشاء نسخة احتياطية من المجلد الحالي إذا كان موجوداً
                                    if os.path.exists(target_dir):
                                        backup_target_dir = f"{target_dir}_backup_{int(time.time())}"
                                        logger.info(f"Creating backup of existing directory {target_dir} to {backup_target_dir}")
                                        shutil.move(target_dir, backup_target_dir)

                                    # إنشاء المجلد الهدف إذا لم يكن موجوداً
                                    os.makedirs(os.path.dirname(target_dir), exist_ok=True)

                                    # نسخ المجلد من المصدر
                                    shutil.copytree(dir_import_path, target_dir)
                                    imported_items['directories'].append(dir_name)

                logger.info(f"Data imported successfully from {source_dir}")
                return {
                    'success': True,
                    'imported_items': imported_items
                }
            else:
                raise FileNotFoundError(f"Export configuration file not found in {source_dir}")
        except Exception as e:
            logger.exception(f"Error importing data: {str(e)}")
            return {
                'successFALSEerror': str(e)
            }

    def get_backups(self):
        """
        الحصول على قائمة النسخ الاحتياطية المتوفرة

        Returns:
            list: قائمة النسخ الاحتياطية
        """
        backups = []

        # البحث عن ملفات النسخ الاحتياطية
        for file_name in os.listdir(self.config['backup_dir']):
            file_path = os.path.join(self.config['backup_dir'], file_name)

            # التحقق من أن الملف هو نسخة احتياطية
            if os.path.isfile(file_path) and any(file_name.endswith(f".tar.{ext}") for ext in ['gz', 'bz2', 'xz']):
                try:
                    # استخراج اسم النسخة الاحتياطية من اسم الملف
                    backup_name = file_name.split('.tar.')[0]

                    # محاولة استخراج ملف التكوين للحصول على معلومات إضافية
                    backup_info = {
                        'name': backup_name,
                        'file_path': file_path,
                        'file_size': os.path.getsize(file_path),
                        'created_at': datetime.datetime.fromtimestamp(os.path.getctime(file_path)).isoformat()
                    }

                    # محاولة استخراج معلومات إضافية من ملف التكوين
                    try:
                        with tarfile.open(file_path, 'r:*') as tar:
                            config_file = None
                            for member in tar.getmembers():
                                if member.name.endswith('backup_config.json'):
                                    config_file = member
                                    break

                            if config_file:
                                config_data = tar.extractfile(config_file).read().decode('utf-8')
                                config = json.loads(config_data)

                                # تحديث معلومات النسخة الاحتياطية
                                backup_info.update({
                                    'descriptionCONFIGGETdescription'),
                                    'created_atCONFIGGETcreated_at', backup_info['created_at']),
                                    'include_dirsCONFIGGETinclude_dirs', []),
                                    'backup_dbsCONFIGGETbackup_dbs', False),
                                    'db_names': list(config.get('db_backup_paths', {}).keys()),
                                    'include_volumesCONFIGGETinclude_volumes', False),
                                    'volume_names': list(config.get('volume_backup_paths', {}).keys()),
                                    'include_bind_mountsCONFIGGETinclude_bind_mounts', False)
                                })
                    except Exception as e:
                        logger.warning(f"Error extracting config from {file_path}: {str(e)}")

                    backups.append(backup_info)
                except Exception as e:
                    logger.warning(f"Error processing backup file {file_path}: {str(e)}")

        # ترتيب النسخ الاحتياطية حسب تاريخ الإنشاء (الأحدث أولاً)
        backups.sort(key=lambda x: x['created_at'], reverse=True)

        return backups

    def get_task_status(self, task_id):
        """
        الحصول على حالة مهمة

        Args:
            task_id (str): معرف المهمة

        Returns:
            dict: معلومات المهمة
        """
        return self.running_tasks.get(task_id)

    def get_all_tasks(self):
        """
        الحصول على جميع المهام

        Returns:
            list: قائمة المهام
        """
        return list(self.running_tasks.values())

    def _copy_directory(self, src, dst, exclude_patterns=None, include_txt_md=False):
        """
        نسخ مجلد مع استبعاد أنماط محددة

        Args:
            src (str): المجلد المصدر
            dst (str): المجلد الهدف
            exclude_patterns (list, optional): قائمة أنماط الملفات المراد استبعادها. Defaults to None.
            include_txt_md (bool, optional): ما إذا كان يجب تضمين جميع ملفات .txt و .md. Defaults to False.
        """
        exclude_patterns = exclude_patterns or []

        # إنشاء المجلد الهدف إذا لم يكن موجوداً
        os.makedirs(dst, exist_ok=True)

        # نسخ الملفات والمجلدات
        for item in os.listdir(src):
            s = os.path.join(src, item)
            d = os.path.join(dst, item)

            # التحقق من أنماط الاستبعاد
            should_exclude = any(Path(s).match(pattern) for pattern in exclude_patterns)

            # تضمين ملفات .txt و .md إذا كان مطلوباً
            if include_txt_md and (s.endswith('.txt') or s.endswith('.md')):
                should_exclude = False

            if should_exclude:
                logger.debug(f"Skipping {s} due to exclude pattern")
                continue

            if os.path.isdir(s):
                self._copy_directory(s, d, exclude_patterns, include_txt_md)
            else:
                shutil.copy2(s, d)

    def _backup_database(self, output_file, db_config):
        """
        نسخ قاعدة البيانات

        Args:
            output_file (str): مسار ملف الإخراج
            db_config (dict): إعدادات قاعدة البيانات

        Raises:
            Exception: في حالة فشل النسخ
        """
        # إنشاء مجلد الإخراج إذا لم يكن موجوداً
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        # إعداد متغيرات البيئة لـ pg_dump
        env = os.environ.copy()
        if db_config.get('password'):
            env['PGPASSWORD'] = db_config['password']

        # إعداد أمر pg_dump
        cmd = [
            'pg_dump""-hDB_CONFIGGEThost', 'localhost""-p', str(db_config.get('port', 5432)),
            '-UDB_CONFIGGETuser', 'postgres""-dDB_CONFIGGETdatabase""-f', output_file,
            '--format=p'  # نص عادي
        ]

        # تنفيذ الأمر
        logger.info(f"Running command: {' '.join(cmd)}")
        process = subprocess.run(cmd, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

        # التحقق من نجاح العملية
        if process.returncode != 0:
            error_message = process.stderr.decode('utf-8')
            raise Exception(f"Database backup failed: {error_message}")

    def _restore_database(self, input_file, db_config):
        """
        استعادة قاعدة البيانات

        Args:
            input_file (str): مسار ملف النسخة الاحتياطية
            db_config (dict): إعدادات قاعدة البيانات

        Raises:
            Exception: في حالة فشل الاستعادة
        """
        # التحقق من وجود ملف النسخة الاحتياطية
        if not os.path.exists(input_file):
            raise FileNotFoundError(f"Database backup file {input_file} not found")

        # إعداد متغيرات البيئة لـ psql
        env = os.environ.copy()
        if db_config.get('password'):
            env['PGPASSWORD'] = db_config['password']

        # إعداد أمر psql
        cmd = [
            'psql""-hDB_CONFIGGEThost', 'localhost""-p', str(db_config.get('port', 5432)),
            '-UDB_CONFIGGETuser', 'postgres""-dDB_CONFIGGETdatabase""-f', input_file
        ]

        # تنفيذ الأمر
        logger.info(f"Running command: {' '.join(cmd)}")
        process = subprocess.run(cmd, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

        # التحقق من نجاح العملية
        if process.returncode != 0:
            error_message = process.stderr.decode('utf-8')
            raise Exception(f"Database restore failed: {error_message}")

    def _backup_docker_volume(self, volume_name, output_file):
        """
        نسخ Docker Volume

        Args:
            volume_name (str): اسم الـ Volume
            output_file (str): مسار ملف الإخراج

        Returns:
            bool: نجاح العملية
        """
        try:
            # إنشاء مجلد الإخراج إذا لم يكن موجوداً
            os.makedirs(os.path.dirname(output_file), exist_ok=True)

            # إنشاء حاوية مؤقتة لنسخ الـ Volume
            cmd = [
                'docker', 'run', '--rm""-v', f"{volume_name}:/source",
                '-v', f"{os.path.dirname(os.path.abspath(output_file))}:/backup",
                'alpine', 'tar', 'cf', f"/backup/{os.path.basename(output_file)}", '-C', '/source', '.'
            ]

            # تنفيذ الأمر
            logger.info(f"Running command: {' '.join(cmd)}")
            process = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

            # التحقق من نجاح العملية
            if process.returncode != 0:
                error_message = process.stderr.decode('utf-8')
                logger.error(f"Docker volume backup failed: {error_message}")
                return False

            return True
        except Exception as e:
            logger.exception(f"Error backing up Docker volume: {str(e)}")
            return False

    def _restore_docker_volume(self, volume_name, input_file):
        """
        استعادة Docker Volume

        Args:
            volume_name (str): اسم الـ Volume
            input_file (str): مسار ملف النسخة الاحتياطية

        Returns:
            bool: نجاح العملية
        """
        try:
            # التحقق من وجود ملف النسخة الاحتياطية
            if not os.path.exists(input_file):
                raise FileNotFoundError(f"Docker volume backup file {input_file} not found")

            # إنشاء الـ Volume إذا لم يكن موجوداً
            cmd_create = ['docker', 'volume', 'create', volume_name]
            subprocess.run(cmd_create, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

            # إنشاء حاوية مؤقتة لاستعادة الـ Volume
            cmd = [
                'docker', 'run', '--rm""-v', f"{volume_name}:/target",
                '-v', f"{os.path.dirname(os.path.abspath(input_file))}:/backup",
                'alpine', 'sh', '-c', f"rm -rf /target/* && tar xf /backup/{os.path.basename(input_file)} -C /target"
            ]

            # تنفيذ الأمر
            logger.info(f"Running command: {' '.join(cmd)}")
            process = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

            # التحقق من نجاح العملية
            if process.returncode != 0:
                error_message = process.stderr.decode('utf-8')
                logger.error(f"Docker volume restore failed: {error_message}")
                return False

            return True
        except Exception as e:
            logger.exception(f"Error restoring Docker volume: {str(e)}")
            return False

    def _cleanup_old_backups(self):
        """
        حذف النسخ الاحتياطية القديمة إذا تجاوز عددها الحد المسموح به
        """
        backups = self.get_backups()

        # التحقق من عدد النسخ الاحتياطية
        if len(backups) > self.config['max_backups']:
            # ترتيب النسخ الاحتياطية حسب تاريخ الإنشاء (الأقدم أولاً)
            backups.sort(key=lambda x: x['created_at'])

            # حذف النسخ الاحتياطية القديمة
            for backup in backups[:len(backups) - self.config['max_backups']]:
                file_path = backup['file_path']
                logger.info(f"Removing old backup {file_path}")
                try:
                    os.remove(file_path)
                except Exception as e:
                    logger.warning(f"Error removing old backup {file_path}: {str(e)}")

    def delete_backup(self, backup_name=None, backup_file=None):
        """
        حذف نسخة احتياطية

        Args:
            backup_name (str, optional): اسم النسخة الاحتياطية. Defaults to None.
            backup_file (str, optional): مسار ملف النسخة الاحتياطية. Defaults to None.

        Returns:
            dict: نتيجة العملية
        """
        try:
            # التحقق من توفر المعلومات المطلوبة
            if not backup_name and not backup_file:
                raise ValueError("Either backup_name or backup_file must be provided")

            # إذا تم تحديد الاسم فقط، ابحث عن الملف المناسب
            if backup_name and not backup_file:
                for ext in ['gz', 'bz2', 'xz']:
                    potential_file = os.path.join(self.config['backup_dir'], f"{backup_name}.tar.{ext}")
                    if os.path.exists(potential_file):
                        backup_file = potential_file
                        break

                if not backup_file:
                    raise FileNotFoundError(f"Backup file for {backup_name} not found")

            # التحقق من وجود الملف
            if not os.path.exists(backup_file):
                raise FileNotFoundError(f"Backup file {backup_file} not found")

            # حذف الملف
            os.remove(backup_file)

            return {
                'successTRUEmessage': f"Backup {backup_name or os.path.basename(backup_file)} deleted successfully"
            }
        except Exception as e:
            logger.exception(f"Error deleting backup: {str(e)}")
            return {
                'successFALSEerror': str(e)
            }

    def update_config(self, new_config):
        """
        تحديث إعدادات النسخ الاحتياطي

        Args:
            new_config (dict): الإعدادات الجديدة

        Returns:
            dict: الإعدادات المحدثة
        """
        # تحديث الإعدادات
        self._deep_update(self.config, new_config)

        # التأكد من وجود مجلدات النسخ الاحتياطي والمؤقت
        os.makedirs(self.config['backup_dir'], exist_ok=True)
        os.makedirs(self.config['temp_dir'], exist_ok=True)

        return self.config

    def get_config(self):
        """
        الحصول على إعدادات النسخ الاحتياطي الحالية

        Returns:
            dict: الإعدادات الحالية
        """
        return self.config

    def schedule_backup(self, schedule_config=None):
        """
        جدولة النسخ الاحتياطي

        Args:
            schedule_config (dict, optional): إعدادات الجدولة. Defaults to None.

        Returns:
            dict: نتيجة العملية
        """
        try:
            # تحديث إعدادات الجدولة
            if schedule_config:
                self.config['schedule'].update(schedule_config)

            # التحقق من تفعيل الجدولة
            if not self.config['schedule']['enabled']:
                return {
                    'success': True,
                    'message': "Backup scheduling is disabled"
                }

            # إنشاء مهمة cron
            frequency = self.config['schedule']['frequency']
            time_parts = self.config['schedule']['time'].split(':')
            hour, minute = int(time_parts[0]), int(time_parts[1])

            cron_expression = ""
            if frequency == 'hourly':
                cron_expression = f"{minute} * * * *"
            elif frequency == 'daily':
                cron_expression = f"{minute} {hour} * * *"
            elif frequency == 'weekly':
                day_of_week = self.config['schedule']['day_of_week']
                cron_expression = f"{minute} {hour} * * {day_of_week}"
            elif frequency == 'monthly':
                day_of_month = self.config['schedule']['day_of_month']
                cron_expression = f"{minute} {hour} {day_of_month} * *"

            # إنشاء ملف cron
            cron_file = os.path.join(os.path.expanduser('~'), 'backup_cron')
            with open(cron_file, 'w') as f:
                f.write(f"{cron_expression} python -m src.modules.backup_restore.schedule_backup\n")

            # تثبيت ملف cron
            cmd = ['crontab', cron_file]
            process = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

            # التحقق من نجاح العملية
            if process.returncode != 0:
                error_message = process.stderr.decode('utf-8')
                raise Exception(f"Failed to install cron job: {error_message}")

            # حذف ملف cron المؤقت
            os.remove(cron_file)

            return {
                'successTRUEmessage': f"Backup scheduled with frequency: {frequency}, time: {self.config['schedule']['time']}"
            }
        except Exception as e:
            logger.exception(f"Error scheduling backup: {str(e)}")
            return {
                'successFALSEerror': str(e)
            }

    def get_storage_info(self):
        """
        الحصول على معلومات التخزين

        Returns:
            dict: معلومات التخزين
        """
        try:
            storage_info = {
                'volumes""bind_mounts""backup_dir""path': self.config['backup_dir'],
                    'total_size': 0,
                    'free_space': 0,
                    'backups_count': 0
                }
            }

            # معلومات Docker Volumes
            if self.config['docker']['use_volumes']:
                cmd = ['docker', 'volume', 'ls', '--format', '{{.Name}}']
                process = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

                if process.returncode == 0:
                    volumes = process.stdout.decode('utf-8').strip().split('\n')

                    for volume in volumes:
                        if volume:
                            # الحصول على معلومات الـ Volume
                            cmd_inspect = ['docker', 'volume', 'inspect', volume]
                            process_inspect = subprocess.run(cmd_inspect, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

                            if process_inspect.returncode == 0:
                                volume_info = json.loads(process_inspect.stdout.decode('utf-8'))

                                if volume_info and len(volume_info) > 0:
                                    storage_info['volumes'].append({
                                        'name': volume,
                                        'driver': volume_info[0].get('Driver'),
                                        'mountpoint': volume_info[0].get('Mountpoint'),
                                        'in_use': volume in self.config['docker']['volumes']
                                    })

            # معلومات Bind Mounts
            if self.config['docker']['use_bind_mounts']:
                for dir_path in self.config['docker']['bind_mounts']:
                    if os.path.exists(dir_path):
                        # الحصول على حجم المجلد
                        total_size = 0
                        for dirpath, dirnames, filenames in os.walk(dir_path):
                            for f in filenames:
                                fp = os.path.join(dirpath, f)
                                if os.path.exists(fp):
                                    total_size += os.path.getsize(fp)

                        storage_info['bind_mounts'].append({
                            'path': dir_path,
                            'size': total_size,
                            'files_count': sum(len(files) for _, _, files in os.walk(dir_path))
                        })

            # معلومات مجلد النسخ الاحتياطي
            if os.path.exists(self.config['backup_dir']):
                # الحصول على المساحة الحرة
                statvfs = os.statvfs(self.config['backup_dir'])
                free_space = statvfs.f_frsize * statvfs.f_bavail

                # الحصول على حجم النسخ الاحتياطية
                total_size = 0
                backups_count = 0

                for file_name in os.listdir(self.config['backup_dir']):
                    file_path = os.path.join(self.config['backup_dir'], file_name)

                    if os.path.isfile(file_path) and any(file_name.endswith(f".tar.{ext}") for ext in ['gz', 'bz2', 'xz']):
                        total_size += os.path.getsize(file_path)
                        backups_count += 1

                storage_info['backup_dir'].update({
                    'total_size': total_size,
                    'free_space': free_space,
                    'backups_count': backups_count
                })

            return storage_info
        except Exception as e:
            logger.exception(f"Error getting storage info: {str(e)}")
            return {
                'error': str(e)
            }
