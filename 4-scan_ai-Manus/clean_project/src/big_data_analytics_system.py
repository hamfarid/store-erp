# File: /home/ubuntu/clean_project/src/big_data_analytics_system.py
"""
Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù: /home/ubuntu/clean_project/src/big_data_analytics_system.py

Ù†Ø¸Ø§Ù… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø© ÙˆØ§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª
ÙŠÙˆÙØ± ØªØ­Ù„ÙŠÙ„Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø© Ù…Ø¹ Ù‚Ø¯Ø±Ø§Øª Ø§Ù„ØªÙ†Ø¨Ø¤ ÙˆØ§Ù„ØªÙˆÙ‚Ø¹
"""

from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional, Union, Tuple, Callable, AsyncGenerator
from dataclasses import dataclass, field
from datetime import datetime, timedelta, date
from enum import Enum
import asyncio
import json
import logging
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.feature_selection import SelectKBest, f_regression
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

class PredictionType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªÙ†Ø¨Ø¤"""
    TIME_SERIES = "time_series"
    REGRESSION = "regression"
    CLASSIFICATION = "classification"
    CLUSTERING = "clustering"
    ANOMALY_DETECTION = "anomaly_detection"

class ModelType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
    LINEAR_REGRESSION = "linear_regression"
    RANDOM_FOREST = "random_forest"
    GRADIENT_BOOSTING = "gradient_boosting"
    NEURAL_NETWORK = "neural_network"
    ARIMA = "arima"
    LSTM = "lstm"
    PROPHET = "prophet"

class DataFrequency(Enum):
    """ØªÙƒØ±Ø§Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    DAILY = "D"
    WEEKLY = "W"
    MONTHLY = "M"
    QUARTERLY = "Q"
    YEARLY = "Y"
    HOURLY = "H"

class TrendDirection(Enum):
    """Ø§ØªØ¬Ø§Ù‡ Ø§Ù„ØªØ±Ù†Ø¯"""
    UPWARD = "upward"
    DOWNWARD = "downward"
    STABLE = "stable"
    VOLATILE = "volatile"

@dataclass
class PredictionRequest:
    """Ø·Ù„Ø¨ Ø§Ù„ØªÙ†Ø¨Ø¤"""
    id: str
    name: str
    prediction_type: PredictionType
    model_type: ModelType
    data_source: str
    target_column: str
    feature_columns: List[str]
    prediction_horizon: int  # Ø¹Ø¯Ø¯ Ø§Ù„ÙØªØ±Ø§Øª Ù„Ù„ØªÙ†Ø¨Ø¤
    confidence_level: float = 0.95
    parameters: Dict[str, Any] = field(default_factory=dict)
    filters: Dict[str, Any] = field(default_factory=dict)

@dataclass
class PredictionResult:
    """Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªÙ†Ø¨Ø¤"""
    request_id: str
    predictions: List[Dict[str, Any]]
    confidence_intervals: List[Dict[str, Any]]
    model_metrics: Dict[str, float]
    feature_importance: Dict[str, float]
    trend_analysis: Dict[str, Any]
    recommendations: List[str]
    charts: List[Dict[str, Any]]
    execution_time: float
    timestamp: datetime

@dataclass
class TrendAnalysis:
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª"""
    direction: TrendDirection
    strength: float  # Ù‚ÙˆØ© Ø§Ù„Ø§ØªØ¬Ø§Ù‡ (0-1)
    seasonality_detected: bool
    seasonal_period: Optional[int]
    change_points: List[datetime]
    growth_rate: float
    volatility: float
    correlation_factors: Dict[str, float]

@dataclass
class AnomalyDetection:
    """ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°"""
    anomalies: List[Dict[str, Any]]
    anomaly_score: float
    threshold: float
    detection_method: str
    confidence: float

class DataPreprocessor:
    """Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self):
        self.logger = logging.getLogger('data_preprocessor')
        self.scalers = {}
        self.encoders = {}
    
    def preprocess_for_prediction(self, df: pd.DataFrame, 
                                 target_column: str,
                                 feature_columns: List[str]) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªÙ†Ø¨Ø¤"""
        try:
            processed_df = df.copy()
            preprocessing_info = {}
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
            missing_info = self._handle_missing_values(processed_df)
            preprocessing_info['missing_values'] = missing_info
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©
            outliers_info = self._handle_outliers(processed_df, feature_columns + [target_column])
            preprocessing_info['outliers'] = outliers_info
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©
            categorical_info = self._encode_categorical_variables(processed_df, feature_columns)
            preprocessing_info['categorical_encoding'] = categorical_info
            
            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            scaling_info = self._scale_features(processed_df, feature_columns)
            preprocessing_info['scaling'] = scaling_info
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙŠØ²Ø§Øª Ø²Ù…Ù†ÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø¹Ù…ÙˆØ¯ ØªØ§Ø±ÙŠØ®
            if 'date' in processed_df.columns or 'timestamp' in processed_df.columns:
                time_features_info = self._create_time_features(processed_df)
                preprocessing_info['time_features'] = time_features_info
            
            # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
            feature_selection_info = self._select_important_features(
                processed_df, target_column, feature_columns
            )
            preprocessing_info['feature_selection'] = feature_selection_info
            
            self.logger.info(f"Data preprocessing completed: {len(processed_df)} rows, {len(processed_df.columns)} columns")
            
            return processed_df, preprocessing_info
            
        except Exception as e:
            self.logger.error(f"Error in data preprocessing: {e}")
            return df, {'error': str(e)}
    
    def _handle_missing_values(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©"""
        missing_info = {}
        
        for column in df.columns:
            missing_count = df[column].isnull().sum()
            missing_percentage = (missing_count / len(df)) * 100
            
            if missing_count > 0:
                if df[column].dtype in ['int64', 'float64']:
                    # Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ø¨Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ØªØ­Ø±Ùƒ Ø£Ùˆ Ø§Ù„ÙˆØ³ÙŠØ·
                    if missing_percentage < 30:
                        df[column] = df[column].fillna(df[column].median())
                        missing_info[column] = f"filled with median ({missing_count} values)"
                    else:
                        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù†Ø³Ø¨Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø¹Ø§Ù„ÙŠØ©ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙ†Ø¨Ø¤
                        df[column] = df[column].interpolate(method='linear')
                        missing_info[column] = f"interpolated ({missing_count} values)"
                else:
                    # Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù†ØµÙŠØ© Ø¨Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹
                    mode_value = df[column].mode().iloc[0] if not df[column].mode().empty else 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'
                    df[column] = df[column].fillna(mode_value)
                    missing_info[column] = f"filled with mode: {mode_value} ({missing_count} values)"
        
        return missing_info
    
    def _handle_outliers(self, df: pd.DataFrame, columns: List[str]) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©"""
        outliers_info = {}
        
        for column in columns:
            if column in df.columns and df[column].dtype in ['int64', 'float64']:
                Q1 = df[column].quantile(0.25)
                Q3 = df[column].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers_mask = (df[column] < lower_bound) | (df[column] > upper_bound)
                outliers_count = outliers_mask.sum()
                
                if outliers_count > 0:
                    # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© Ø¨Ø§Ù„Ø­Ø¯ÙˆØ¯
                    df.loc[df[column] < lower_bound, column] = lower_bound
                    df.loc[df[column] > upper_bound, column] = upper_bound
                    
                    outliers_info[column] = {
                        'count': outliers_count,
                        'percentage': (outliers_count / len(df)) * 100,
                        'bounds': {'lower': lower_bound, 'upper': upper_bound}
                    }
        
        return outliers_info
    
    def _encode_categorical_variables(self, df: pd.DataFrame, feature_columns: List[str]) -> Dict[str, Any]:
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©"""
        encoding_info = {}
        
        for column in feature_columns:
            if column in df.columns and df[column].dtype == 'object':
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… Label Encoding Ù„Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©
                encoder = LabelEncoder()
                df[column] = encoder.fit_transform(df[column].astype(str))
                
                self.encoders[column] = encoder
                encoding_info[column] = {
                    'method': 'label_encoding',
                    'classes': encoder.classes_.tolist()
                }
        
        return encoding_info
    
    def _scale_features(self, df: pd.DataFrame, feature_columns: List[str]) -> Dict[str, Any]:
        """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª"""
        scaling_info = {}
        
        numeric_columns = [col for col in feature_columns 
                          if col in df.columns and df[col].dtype in ['int64', 'float64']]
        
        if numeric_columns:
            scaler = StandardScaler()
            df[numeric_columns] = scaler.fit_transform(df[numeric_columns])
            
            self.scalers['features'] = scaler
            scaling_info = {
                'method': 'standard_scaling',
                'columns': numeric_columns,
                'mean': scaler.mean_.tolist(),
                'scale': scaler.scale_.tolist()
            }
        
        return scaling_info
    
    def _create_time_features(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙŠØ²Ø§Øª Ø²Ù…Ù†ÙŠØ©"""
        time_features_info = {}
        
        date_column = 'date' if 'date' in df.columns else 'timestamp'
        
        if date_column in df.columns:
            df[date_column] = pd.to_datetime(df[date_column])
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙŠØ²Ø§Øª Ø²Ù…Ù†ÙŠØ©
            df['year'] = df[date_column].dt.year
            df['month'] = df[date_column].dt.month
            df['day'] = df[date_column].dt.day
            df['dayofweek'] = df[date_column].dt.dayofweek
            df['quarter'] = df[date_column].dt.quarter
            df['is_weekend'] = (df[date_column].dt.dayofweek >= 5).astype(int)
            
            # Ù…ÙŠØ²Ø§Øª Ø¯ÙˆØ±ÙŠØ©
            df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
            df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
            df['day_sin'] = np.sin(2 * np.pi * df['day'] / 31)
            df['day_cos'] = np.cos(2 * np.pi * df['day'] / 31)
            
            time_features_info = {
                'created_features': ['year', 'month', 'day', 'dayofweek', 'quarter', 
                                   'is_weekend', 'month_sin', 'month_cos', 'day_sin', 'day_cos'],
                'date_column': date_column
            }
        
        return time_features_info
    
    def _select_important_features(self, df: pd.DataFrame, 
                                  target_column: str, 
                                  feature_columns: List[str]) -> Dict[str, Any]:
        """Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©"""
        try:
            if target_column not in df.columns:
                return {'error': f'Target column {target_column} not found'}
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø©
            available_features = [col for col in feature_columns if col in df.columns]
            numeric_features = [col for col in available_features 
                              if df[col].dtype in ['int64', 'float64']]
            
            if len(numeric_features) < 2:
                return {'message': 'Not enough numeric features for selection'}
            
            X = df[numeric_features]
            y = df[target_column]
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‚ÙŠÙ… Ù…ÙÙ‚ÙˆØ¯Ø©
            mask = ~(X.isnull().any(axis=1) | y.isnull())
            X = X[mask]
            y = y[mask]
            
            if len(X) == 0:
                return {'error': 'No valid data after removing missing values'}
            
            # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø§Ù„Ù…ÙŠØ²Ø§Øª
            k = min(10, len(numeric_features))  # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ 10 Ù…ÙŠØ²Ø§Øª Ø£Ùˆ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…ØªØ§Ø­
            selector = SelectKBest(score_func=f_regression, k=k)
            X_selected = selector.fit_transform(X, y)
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©
            selected_features = [numeric_features[i] for i in selector.get_support(indices=True)]
            feature_scores = dict(zip(numeric_features, selector.scores_))
            
            return {
                'selected_features': selected_features,
                'feature_scores': feature_scores,
                'selection_method': 'f_regression'
            }
            
        except Exception as e:
            self.logger.error(f"Error in feature selection: {e}")
            return {'error': str(e)}

class PredictionEngine:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙ†Ø¨Ø¤"""
    
    def __init__(self):
        self.logger = logging.getLogger('prediction_engine')
        self.models = {}
        self.preprocessor = DataPreprocessor()
    
    async def train_model(self, request: PredictionRequest, df: pd.DataFrame) -> Dict[str, Any]:
        """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        try:
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            processed_df, preprocessing_info = self.preprocessor.preprocess_for_prediction(
                df, request.target_column, request.feature_columns
            )
            
            if 'error' in preprocessing_info:
                return {'error': preprocessing_info['error']}
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨
            X, y = self._prepare_training_data(processed_df, request)
            
            if X is None or y is None:
                return {'error': 'Failed to prepare training data'}
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            
            # Ø§Ø®ØªÙŠØ§Ø± ÙˆØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            model = self._create_model(request.model_type, request.parameters)
            model.fit(X_train, y_train)
            
            # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            y_pred = model.predict(X_test)
            metrics = self._calculate_metrics(y_test, y_pred)
            
            # Ø­Ø³Ø§Ø¨ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª
            feature_importance = self._calculate_feature_importance(model, X.columns)
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            self.models[request.id] = {
                'model': model,
                'preprocessing_info': preprocessing_info,
                'feature_columns': X.columns.tolist(),
                'metrics': metrics,
                'feature_importance': feature_importance,
                'trained_at': datetime.now()
            }
            
            return {
                'model_id': request.id,
                'metrics': metrics,
                'feature_importance': feature_importance,
                'preprocessing_info': preprocessing_info
            }
            
        except Exception as e:
            self.logger.error(f"Error training model {request.id}: {e}")
            return {'error': str(e)}
    
    async def make_prediction(self, request: PredictionRequest, df: pd.DataFrame) -> PredictionResult:
        """Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤"""
        start_time = datetime.now()
        
        try:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£Ùˆ ØªØ¯Ø±ÙŠØ¨Ù‡
            if request.id not in self.models:
                training_result = await self.train_model(request, df)
                if 'error' in training_result:
                    return PredictionResult(
                        request_id=request.id,
                        predictions=[],
                        confidence_intervals=[],
                        model_metrics={},
                        feature_importance={},
                        trend_analysis={},
                        recommendations=[f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {training_result['error']}"],
                        charts=[],
                        execution_time=0,
                        timestamp=datetime.now()
                    )
            
            model_info = self.models[request.id]
            model = model_info['model']
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªÙ†Ø¨Ø¤
            processed_df, _ = self.preprocessor.preprocess_for_prediction(
                df, request.target_column, request.feature_columns
            )
            
            # ØªØ­Ø¶ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙ†Ø¨Ø¤
            X_pred = self._prepare_prediction_data(processed_df, request, model_info)
            
            if X_pred is None:
                return self._create_error_result(request.id, "ÙØ´Ù„ ÙÙŠ ØªØ­Ø¶ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙ†Ø¨Ø¤")
            
            # Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤
            predictions = model.predict(X_pred)
            
            # Ø­Ø³Ø§Ø¨ ÙØªØ±Ø§Øª Ø§Ù„Ø«Ù‚Ø©
            confidence_intervals = self._calculate_confidence_intervals(
                model, X_pred, predictions, request.confidence_level
            )
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª
            trend_analysis = await self._analyze_trends(df, request.target_column, predictions)
            
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª
            recommendations = self._generate_recommendations(
                predictions, trend_analysis, model_info['metrics']
            )
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø®Ø·Ø·Ø§Øª
            charts = await self._create_prediction_charts(
                df, predictions, confidence_intervals, request
            )
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            prediction_results = []
            for i, pred in enumerate(predictions):
                prediction_results.append({
                    'index': i,
                    'predicted_value': float(pred),
                    'confidence_lower': confidence_intervals[i]['lower'] if i < len(confidence_intervals) else None,
                    'confidence_upper': confidence_intervals[i]['upper'] if i < len(confidence_intervals) else None,
                    'prediction_date': (datetime.now() + timedelta(days=i)).isoformat()
                })
            
            execution_time = (datetime.now() - start_time).total_seconds()
            
            return PredictionResult(
                request_id=request.id,
                predictions=prediction_results,
                confidence_intervals=confidence_intervals,
                model_metrics=model_info['metrics'],
                feature_importance=model_info['feature_importance'],
                trend_analysis=trend_analysis,
                recommendations=recommendations,
                charts=charts,
                execution_time=execution_time,
                timestamp=datetime.now()
            )
            
        except Exception as e:
            self.logger.error(f"Error making prediction {request.id}: {e}")
            return self._create_error_result(request.id, str(e))
    
    def _create_model(self, model_type: ModelType, parameters: Dict[str, Any]):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        if model_type == ModelType.LINEAR_REGRESSION:
            return LinearRegression(**parameters)
        elif model_type == ModelType.RANDOM_FOREST:
            default_params = {'n_estimators': 100, 'random_state': 42}
            default_params.update(parameters)
            return RandomForestRegressor(**default_params)
        elif model_type == ModelType.GRADIENT_BOOSTING:
            default_params = {'n_estimators': 100, 'random_state': 42}
            default_params.update(parameters)
            return GradientBoostingRegressor(**default_params)
        else:
            # Ø§ÙØªØ±Ø§Ø¶ÙŠ: Random Forest
            return RandomForestRegressor(n_estimators=100, random_state=42)
    
    def _prepare_training_data(self, df: pd.DataFrame, request: PredictionRequest) -> Tuple[pd.DataFrame, pd.Series]:
        """ØªØ­Ø¶ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        try:
            if request.target_column not in df.columns:
                self.logger.error(f"Target column {request.target_column} not found")
                return None, None
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©
            available_features = [col for col in request.feature_columns if col in df.columns]
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ØªØ§Ø­Ø©
            time_features = ['year', 'month', 'day', 'dayofweek', 'quarter', 'is_weekend',
                           'month_sin', 'month_cos', 'day_sin', 'day_cos']
            available_features.extend([col for col in time_features if col in df.columns])
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
            available_features = list(set(available_features))
            
            if not available_features:
                self.logger.error("No valid features found")
                return None, None
            
            X = df[available_features]
            y = df[request.target_column]
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‚ÙŠÙ… Ù…ÙÙ‚ÙˆØ¯Ø©
            mask = ~(X.isnull().any(axis=1) | y.isnull())
            X = X[mask]
            y = y[mask]
            
            if len(X) == 0:
                self.logger.error("No valid data after removing missing values")
                return None, None
            
            return X, y
            
        except Exception as e:
            self.logger.error(f"Error preparing training data: {e}")
            return None, None
    
    def _prepare_prediction_data(self, df: pd.DataFrame, request: PredictionRequest, 
                               model_info: Dict[str, Any]) -> Optional[pd.DataFrame]:
        """ØªØ­Ø¶ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙ†Ø¨Ø¤"""
        try:
            feature_columns = model_info['feature_columns']
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ
            if request.prediction_type == PredictionType.TIME_SERIES:
                # Ù„Ù„Ø³Ù„Ø§Ø³Ù„ Ø§Ù„Ø²Ù…Ù†ÙŠØ©ØŒ Ø¥Ù†Ø´Ø§Ø¡ ØªÙˆØ§Ø±ÙŠØ® Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©
                last_date = df['date'].max() if 'date' in df.columns else datetime.now()
                future_dates = pd.date_range(
                    start=last_date + timedelta(days=1),
                    periods=request.prediction_horizon,
                    freq='D'
                )
                
                # Ø¥Ù†Ø´Ø§Ø¡ DataFrame Ù„Ù„ØªÙ†Ø¨Ø¤
                future_df = pd.DataFrame({'date': future_dates})
                
                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ©
                future_df['year'] = future_df['date'].dt.year
                future_df['month'] = future_df['date'].dt.month
                future_df['day'] = future_df['date'].dt.day
                future_df['dayofweek'] = future_df['date'].dt.dayofweek
                future_df['quarter'] = future_df['date'].dt.quarter
                future_df['is_weekend'] = (future_df['date'].dt.dayofweek >= 5).astype(int)
                
                # Ù…ÙŠØ²Ø§Øª Ø¯ÙˆØ±ÙŠØ©
                future_df['month_sin'] = np.sin(2 * np.pi * future_df['month'] / 12)
                future_df['month_cos'] = np.cos(2 * np.pi * future_df['month'] / 12)
                future_df['day_sin'] = np.sin(2 * np.pi * future_df['day'] / 31)
                future_df['day_cos'] = np.cos(2 * np.pi * future_df['day'] / 31)
                
                # Ù…Ù„Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰ Ø¨Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ø£Ùˆ Ø§Ù„Ù…ØªÙˆØ³Ø·
                for col in feature_columns:
                    if col not in future_df.columns:
                        if col in df.columns:
                            if df[col].dtype in ['int64', 'float64']:
                                future_df[col] = df[col].mean()
                            else:
                                future_df[col] = df[col].mode().iloc[0] if not df[col].mode().empty else 0
                        else:
                            future_df[col] = 0
                
                X_pred = future_df[feature_columns]
            else:
                # Ù„Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø®Ø±Ù‰ØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¢Ø®Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©
                X_pred = df[feature_columns].tail(request.prediction_horizon)
            
            return X_pred
            
        except Exception as e:
            self.logger.error(f"Error preparing prediction data: {e}")
            return None
    
    def _calculate_metrics(self, y_true, y_pred) -> Dict[str, float]:
        """Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        try:
            return {
                'mse': float(mean_squared_error(y_true, y_pred)),
                'rmse': float(np.sqrt(mean_squared_error(y_true, y_pred))),
                'mae': float(mean_absolute_error(y_true, y_pred)),
                'r2': float(r2_score(y_true, y_pred)),
                'mape': float(np.mean(np.abs((y_true - y_pred) / y_true)) * 100) if np.all(y_true != 0) else 0
            }
        except Exception as e:
            self.logger.error(f"Error calculating metrics: {e}")
            return {}
    
    def _calculate_feature_importance(self, model, feature_names) -> Dict[str, float]:
        """Ø­Ø³Ø§Ø¨ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª"""
        try:
            if hasattr(model, 'feature_importances_'):
                importance = model.feature_importances_
                return dict(zip(feature_names, [float(imp) for imp in importance]))
            elif hasattr(model, 'coef_'):
                importance = np.abs(model.coef_)
                return dict(zip(feature_names, [float(imp) for imp in importance]))
            else:
                return {}
        except Exception as e:
            self.logger.error(f"Error calculating feature importance: {e}")
            return {}
    
    def _calculate_confidence_intervals(self, model, X_pred, predictions, confidence_level) -> List[Dict[str, float]]:
        """Ø­Ø³Ø§Ø¨ ÙØªØ±Ø§Øª Ø§Ù„Ø«Ù‚Ø©"""
        try:
            # ØªÙ‚Ø¯ÙŠØ± ØªÙ‚Ø±ÙŠØ¨ÙŠ Ù„ÙØªØ±Ø§Øª Ø§Ù„Ø«Ù‚Ø©
            # ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙØ¹Ù„ÙŠØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±Ù‚ Ø£ÙƒØ«Ø± ØªØ·ÙˆØ±Ø§Ù‹
            
            std_error = np.std(predictions) * 0.1  # ØªÙ‚Ø¯ÙŠØ± ØªÙ‚Ø±ÙŠØ¨ÙŠ Ù„Ù„Ø®Ø·Ø£ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ
            z_score = 1.96 if confidence_level == 0.95 else 2.58  # Ù„Ù„Ø«Ù‚Ø© 95% Ø£Ùˆ 99%
            
            margin_of_error = z_score * std_error
            
            intervals = []
            for pred in predictions:
                intervals.append({
                    'lower': float(pred - margin_of_error),
                    'upper': float(pred + margin_of_error),
                    'confidence_level': confidence_level
                })
            
            return intervals
            
        except Exception as e:
            self.logger.error(f"Error calculating confidence intervals: {e}")
            return []
    
    async def _analyze_trends(self, df: pd.DataFrame, target_column: str, 
                            predictions: np.ndarray) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª"""
        try:
            trend_analysis = {}
            
            if target_column in df.columns and len(df) > 1:
                values = df[target_column].values
                
                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø¹Ø§Ù…
                x = np.arange(len(values))
                slope = np.polyfit(x, values, 1)[0]
                
                if slope > 0.1:
                    direction = TrendDirection.UPWARD
                elif slope < -0.1:
                    direction = TrendDirection.DOWNWARD
                else:
                    direction = TrendDirection.STABLE
                
                # Ø­Ø³Ø§Ø¨ Ù‚ÙˆØ© Ø§Ù„Ø§ØªØ¬Ø§Ù‡
                correlation = np.corrcoef(x, values)[0, 1]
                strength = abs(correlation)
                
                # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙ‚Ù„Ø¨Ø§Øª
                volatility = np.std(values) / np.mean(values) if np.mean(values) != 0 else 0
                
                # Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ù…Ùˆ
                if len(values) > 1:
                    growth_rate = ((values[-1] - values[0]) / values[0]) * 100 if values[0] != 0 else 0
                else:
                    growth_rate = 0
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
                pred_trend = "ØµØ§Ø¹Ø¯" if np.mean(predictions) > values[-1] else "Ù‡Ø§Ø¨Ø·"
                
                trend_analysis = {
                    'direction': direction.value,
                    'strength': float(strength),
                    'volatility': float(volatility),
                    'growth_rate': float(growth_rate),
                    'prediction_trend': pred_trend,
                    'current_value': float(values[-1]),
                    'predicted_average': float(np.mean(predictions)),
                    'trend_change': float(np.mean(predictions) - values[-1])
                }
            
            return trend_analysis
            
        except Exception as e:
            self.logger.error(f"Error analyzing trends: {e}")
            return {}
    
    def _generate_recommendations(self, predictions: np.ndarray, 
                                trend_analysis: Dict[str, Any],
                                model_metrics: Dict[str, float]) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª"""
        recommendations = []
        
        try:
            # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§ØªØ¬Ø§Ù‡
            if 'direction' in trend_analysis:
                direction = trend_analysis['direction']
                if direction == TrendDirection.UPWARD.value:
                    recommendations.append("ğŸ“ˆ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ ØµØ§Ø¹Ø¯ - ÙŠÙÙ†ØµØ­ Ø¨Ø§Ù„Ø§Ø³ØªØ«Ù…Ø§Ø± Ø£Ùˆ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬")
                elif direction == TrendDirection.DOWNWARD.value:
                    recommendations.append("ğŸ“‰ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ù‡Ø§Ø¨Ø· - ÙŠÙÙ†ØµØ­ Ø¨Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª")
                else:
                    recommendations.append("ğŸ“Š Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ù…Ø³ØªÙ‚Ø± - Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ")
            
            # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ‚Ù„Ø¨Ø§Øª
            if 'volatility' in trend_analysis:
                volatility = trend_analysis['volatility']
                if volatility > 0.3:
                    recommendations.append("âš ï¸ ØªÙ‚Ù„Ø¨Ø§Øª Ø¹Ø§Ù„ÙŠØ© - ØªØ·Ø¨ÙŠÙ‚ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø®Ø§Ø·Ø±")
                elif volatility < 0.1:
                    recommendations.append("âœ… Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø¬ÙŠØ¯ - Ø¨ÙŠØ¦Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„ØªØ®Ø·ÙŠØ· Ø·ÙˆÙŠÙ„ Ø§Ù„Ù…Ø¯Ù‰")
            
            # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            if 'r2' in model_metrics:
                r2 = model_metrics['r2']
                if r2 > 0.8:
                    recommendations.append("ğŸ¯ Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ - ÙŠÙ…ÙƒÙ† Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª")
                elif r2 < 0.5:
                    recommendations.append("âš ï¸ Ø¯Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø© - ÙŠÙÙ†ØµØ­ Ø¨Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
            
            # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
            if len(predictions) > 0:
                pred_change = (predictions[-1] - predictions[0]) / predictions[0] * 100 if predictions[0] != 0 else 0
                if abs(pred_change) > 20:
                    recommendations.append(f"ğŸ”„ ØªØºÙŠÙŠØ± ÙƒØ¨ÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ({pred_change:.1f}%) - Ø§Ù„ØªØ­Ø¶ÙŠØ± Ù„Ù„ØªÙƒÙŠÙ")
            
            # ØªÙˆØµÙŠØ§Øª Ø¹Ø§Ù…Ø©
            recommendations.append("ğŸ“Š Ù…Ø±Ø§Ù‚Ø¨Ø© Ø¯ÙˆØ±ÙŠØ© Ù„Ù„ØªÙ†Ø¨Ø¤Ø§Øª ÙˆØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")
            recommendations.append("ğŸ“ˆ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ù…Ø¤Ø«Ø±Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø©")
            
            return recommendations[:6]  # Ø£Ù‚ØµÙ‰ 6 ØªÙˆØµÙŠØ§Øª
            
        except Exception as e:
            self.logger.error(f"Error generating recommendations: {e}")
            return ["âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª"]
    
    async def _create_prediction_charts(self, df: pd.DataFrame, predictions: np.ndarray,
                                      confidence_intervals: List[Dict[str, float]],
                                      request: PredictionRequest) -> List[Dict[str, Any]]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø·Ø·Ø§Øª Ø§Ù„ØªÙ†Ø¨Ø¤"""
        charts = []
        
        try:
            # Ù…Ø®Ø·Ø· Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø²Ù…Ù†ÙŠ
            if 'date' in df.columns and request.target_column in df.columns:
                # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©
                historical_dates = df['date'].tolist()
                historical_values = df[request.target_column].tolist()
                
                # Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©
                last_date = df['date'].max()
                future_dates = pd.date_range(
                    start=last_date + timedelta(days=1),
                    periods=len(predictions),
                    freq='D'
                ).tolist()
                
                # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø®Ø·Ø·
                fig = go.Figure()
                
                # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©
                fig.add_trace(go.Scatter(
                    x=historical_dates,
                    y=historical_values,
                    mode='lines+markers',
                    name='Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©',
                    line=dict(color='blue')
                ))
                
                # Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
                fig.add_trace(go.Scatter(
                    x=future_dates,
                    y=predictions.tolist(),
                    mode='lines+markers',
                    name='Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª',
                    line=dict(color='red', dash='dash')
                ))
                
                # ÙØªØ±Ø§Øª Ø§Ù„Ø«Ù‚Ø©
                if confidence_intervals:
                    upper_bounds = [ci['upper'] for ci in confidence_intervals]
                    lower_bounds = [ci['lower'] for ci in confidence_intervals]
                    
                    fig.add_trace(go.Scatter(
                        x=future_dates + future_dates[::-1],
                        y=upper_bounds + lower_bounds[::-1],
                        fill='toself',
                        fillcolor='rgba(255,0,0,0.2)',
                        line=dict(color='rgba(255,255,255,0)'),
                        name='ÙØªØ±Ø© Ø§Ù„Ø«Ù‚Ø©',
                        showlegend=True
                    ))
                
                fig.update_layout(
                    title=f'ØªÙ†Ø¨Ø¤ {request.target_column}',
                    xaxis_title='Ø§Ù„ØªØ§Ø±ÙŠØ®',
                    yaxis_title=request.target_column,
                    font=dict(family="Arial Unicode MS, Tahoma", size=12),
                    hovermode='x unified'
                )
                
                charts.append({
                    'type': 'prediction_timeline',
                    'title': f'ØªÙ†Ø¨Ø¤ {request.target_column}',
                    'data': fig.to_json(),
                    'config': {'displayModeBar': True, 'responsive': True}
                })
            
            # Ù…Ø®Ø·Ø· ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
            fig_dist = go.Figure()
            fig_dist.add_trace(go.Histogram(
                x=predictions,
                nbinsx=20,
                name='ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª',
                marker_color='lightblue'
            ))
            
            fig_dist.update_layout(
                title='ØªÙˆØ²ÙŠØ¹ Ù‚ÙŠÙ… Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª',
                xaxis_title='Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…ØªÙ†Ø¨Ø£ Ø¨Ù‡Ø§',
                yaxis_title='Ø§Ù„ØªÙƒØ±Ø§Ø±',
                font=dict(family="Arial Unicode MS, Tahoma", size=12)
            )
            
            charts.append({
                'type': 'prediction_distribution',
                'title': 'ØªÙˆØ²ÙŠØ¹ Ù‚ÙŠÙ… Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª',
                'data': fig_dist.to_json(),
                'config': {'displayModeBar': True, 'responsive': True}
            })
            
            return charts
            
        except Exception as e:
            self.logger.error(f"Error creating prediction charts: {e}")
            return []
    
    def _create_error_result(self, request_id: str, error_message: str) -> PredictionResult:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù†ØªÙŠØ¬Ø© Ø®Ø·Ø£"""
        return PredictionResult(
            request_id=request_id,
            predictions=[],
            confidence_intervals=[],
            model_metrics={},
            feature_importance={},
            trend_analysis={},
            recommendations=[f"âŒ Ø®Ø·Ø£: {error_message}"],
            charts=[],
            execution_time=0,
            timestamp=datetime.now()
        )

class AnomalyDetector:
    """ÙƒØ§Ø´Ù Ø§Ù„Ø´Ø°ÙˆØ°"""
    
    def __init__(self):
        self.logger = logging.getLogger('anomaly_detector')
    
    async def detect_anomalies(self, df: pd.DataFrame, 
                             target_column: str,
                             method: str = "isolation_forest") -> AnomalyDetection:
        """ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            if target_column not in df.columns:
                return AnomalyDetection(
                    anomalies=[],
                    anomaly_score=0.0,
                    threshold=0.0,
                    detection_method=method,
                    confidence=0.0
                )
            
            values = df[target_column].values.reshape(-1, 1)
            
            if method == "isolation_forest":
                detector = IsolationForest(contamination=0.1, random_state=42)
                anomaly_labels = detector.fit_predict(values)
                anomaly_scores = detector.decision_function(values)
                
                # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø´Ø°ÙˆØ°
                anomalies = []
                for i, (label, score) in enumerate(zip(anomaly_labels, anomaly_scores)):
                    if label == -1:  # Ø´Ø°ÙˆØ°
                        anomaly_data = {
                            'index': i,
                            'value': float(values[i][0]),
                            'anomaly_score': float(score),
                            'severity': 'high' if score < -0.5 else 'medium'
                        }
                        
                        if 'date' in df.columns:
                            anomaly_data['date'] = df.iloc[i]['date'].isoformat() if pd.notna(df.iloc[i]['date']) else None
                        
                        anomalies.append(anomaly_data)
                
                overall_score = float(np.mean(np.abs(anomaly_scores)))
                threshold = float(np.percentile(anomaly_scores, 10))  # Ø£Ø³ÙˆØ£ 10%
                confidence = min(1.0, overall_score * 2)  # ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø«Ù‚Ø©
                
            elif method == "statistical":
                # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ© (Z-score)
                mean_val = np.mean(values)
                std_val = np.std(values)
                z_scores = np.abs((values.flatten() - mean_val) / std_val)
                
                threshold = 3.0  # 3 Ø§Ù†Ø­Ø±Ø§ÙØ§Øª Ù…Ø¹ÙŠØ§Ø±ÙŠØ©
                anomaly_indices = np.where(z_scores > threshold)[0]
                
                anomalies = []
                for i in anomaly_indices:
                    anomaly_data = {
                        'index': i,
                        'value': float(values[i][0]),
                        'z_score': float(z_scores[i]),
                        'severity': 'high' if z_scores[i] > 4 else 'medium'
                    }
                    
                    if 'date' in df.columns:
                        anomaly_data['date'] = df.iloc[i]['date'].isoformat() if pd.notna(df.iloc[i]['date']) else None
                    
                    anomalies.append(anomaly_data)
                
                overall_score = float(np.mean(z_scores))
                confidence = min(1.0, overall_score / 3.0)
            
            else:
                return AnomalyDetection(
                    anomalies=[],
                    anomaly_score=0.0,
                    threshold=0.0,
                    detection_method=method,
                    confidence=0.0
                )
            
            return AnomalyDetection(
                anomalies=anomalies,
                anomaly_score=overall_score,
                threshold=threshold,
                detection_method=method,
                confidence=confidence
            )
            
        except Exception as e:
            self.logger.error(f"Error detecting anomalies: {e}")
            return AnomalyDetection(
                anomalies=[],
                anomaly_score=0.0,
                threshold=0.0,
                detection_method=method,
                confidence=0.0
            )

class BigDataAnalyticsEngine:
    """Ù…Ø­Ø±Ùƒ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø©"""
    
    def __init__(self):
        self.prediction_engine = PredictionEngine()
        self.anomaly_detector = AnomalyDetector()
        self.logger = logging.getLogger('big_data_analytics_engine')
    
    async def analyze_big_data(self, data_source: str, 
                             analysis_type: str = "comprehensive") -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø©"""
        try:
            # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ù…Ø­Ø§ÙƒØ§Ø©)
            df = await self._fetch_big_data(data_source)
            
            if df.empty:
                return {'error': 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ§Ø­Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„'}
            
            analysis_results = {
                'data_summary': self._get_data_summary(df),
                'timestamp': datetime.now().isoformat()
            }
            
            if analysis_type in ["comprehensive", "trends"]:
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª
                trends_analysis = await self._analyze_comprehensive_trends(df)
                analysis_results['trends_analysis'] = trends_analysis
            
            if analysis_type in ["comprehensive", "anomalies"]:
                # ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°
                anomalies_analysis = await self._detect_comprehensive_anomalies(df)
                analysis_results['anomalies_analysis'] = anomalies_analysis
            
            if analysis_type in ["comprehensive", "predictions"]:
                # Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
                predictions_analysis = await self._generate_comprehensive_predictions(df)
                analysis_results['predictions_analysis'] = predictions_analysis
            
            if analysis_type in ["comprehensive", "clustering"]:
                # Ø§Ù„ØªØ¬Ù…ÙŠØ¹
                clustering_analysis = await self._perform_clustering_analysis(df)
                analysis_results['clustering_analysis'] = clustering_analysis
            
            return analysis_results
            
        except Exception as e:
            self.logger.error(f"Error in big data analysis: {e}")
            return {'error': str(e)}
    
    async def _fetch_big_data(self, data_source: str) -> pd.DataFrame:
        """Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø©"""
        try:
            # Ù…Ø­Ø§ÙƒØ§Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø¶Ø®Ù…Ø©
            np.random.seed(42)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø²Ù…Ù†ÙŠØ© Ù„Ø³Ù†Ø© ÙƒØ§Ù…Ù„Ø©
            dates = pd.date_range(start='2024-01-01', end='2024-12-31', freq='H')
            n_records = len(dates)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªÙ†ÙˆØ¹Ø©
            data = {
                'timestamp': dates,
                'sales': np.random.normal(1000, 200, n_records) + 
                        np.sin(np.arange(n_records) * 2 * np.pi / (24*7)) * 100 +  # Ø£Ø³Ø¨ÙˆØ¹ÙŠ
                        np.sin(np.arange(n_records) * 2 * np.pi / 24) * 50,  # ÙŠÙˆÙ…ÙŠ
                'customers': np.random.poisson(50, n_records),
                'revenue': np.random.normal(5000, 1000, n_records),
                'temperature': 20 + 10 * np.sin(np.arange(n_records) * 2 * np.pi / (24*365)) + np.random.normal(0, 2, n_records),
                'humidity': 50 + 20 * np.sin(np.arange(n_records) * 2 * np.pi / (24*180)) + np.random.normal(0, 5, n_records),
                'category': np.random.choice(['A', 'B', 'C', 'D'], n_records),
                'region': np.random.choice(['Ø§Ù„Ø´Ù…Ø§Ù„', 'Ø§Ù„Ø¬Ù†ÙˆØ¨', 'Ø§Ù„Ø´Ø±Ù‚', 'Ø§Ù„ØºØ±Ø¨', 'Ø§Ù„ÙˆØ³Ø·'], n_records),
                'product_id': np.random.randint(1, 100, n_records),
                'user_id': np.random.randint(1, 1000, n_records)
            }
            
            df = pd.DataFrame(data)
            
            # Ø¥Ø¶Ø§ÙØ© Ø¨Ø¹Ø¶ Ø§Ù„Ø´Ø°ÙˆØ°
            anomaly_indices = np.random.choice(len(df), size=int(len(df) * 0.02), replace=False)
            df.loc[anomaly_indices, 'sales'] *= np.random.uniform(3, 5, len(anomaly_indices))
            
            return df
            
        except Exception as e:
            self.logger.error(f"Error fetching big data: {e}")
            return pd.DataFrame()
    
    def _get_data_summary(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Ù…Ù„Ø®Øµ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            categorical_columns = df.select_dtypes(include=['object']).columns
            
            summary = {
                'total_records': len(df),
                'total_columns': len(df.columns),
                'numeric_columns': len(numeric_columns),
                'categorical_columns': len(categorical_columns),
                'missing_values': df.isnull().sum().sum(),
                'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024,
                'date_range': {
                    'start': df['timestamp'].min().isoformat() if 'timestamp' in df.columns else None,
                    'end': df['timestamp'].max().isoformat() if 'timestamp' in df.columns else None
                }
            }
            
            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ©
            if len(numeric_columns) > 0:
                summary['numeric_stats'] = df[numeric_columns].describe().to_dict()
            
            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ÙØ¦ÙˆÙŠØ©
            if len(categorical_columns) > 0:
                summary['categorical_stats'] = {}
                for col in categorical_columns:
                    summary['categorical_stats'][col] = {
                        'unique_values': df[col].nunique(),
                        'most_frequent': df[col].mode().iloc[0] if not df[col].mode().empty else None
                    }
            
            return summary
            
        except Exception as e:
            self.logger.error(f"Error getting data summary: {e}")
            return {}
    
    async def _analyze_comprehensive_trends(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª"""
        try:
            trends_analysis = {}
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            
            for column in numeric_columns:
                if column in df.columns:
                    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø²Ù…Ù†ÙŠ
                    if 'timestamp' in df.columns:
                        time_series_data = df.groupby(df['timestamp'].dt.date)[column].mean()
                        
                        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§ØªØ¬Ø§Ù‡
                        x = np.arange(len(time_series_data))
                        y = time_series_data.values
                        
                        if len(y) > 1:
                            slope = np.polyfit(x, y, 1)[0]
                            correlation = np.corrcoef(x, y)[0, 1] if len(y) > 1 else 0
                            
                            # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø§ØªØ¬Ø§Ù‡
                            if slope > 0.1:
                                direction = "ØµØ§Ø¹Ø¯"
                            elif slope < -0.1:
                                direction = "Ù‡Ø§Ø¨Ø·"
                            else:
                                direction = "Ù…Ø³ØªÙ‚Ø±"
                            
                            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙˆØ³Ù…ÙŠØ©
                            seasonality = self._detect_seasonality_advanced(time_series_data)
                            
                            trends_analysis[column] = {
                                'direction': direction,
                                'slope': float(slope),
                                'strength': float(abs(correlation)),
                                'seasonality': seasonality,
                                'volatility': float(np.std(y) / np.mean(y)) if np.mean(y) != 0 else 0,
                                'growth_rate': float(((y[-1] - y[0]) / y[0]) * 100) if y[0] != 0 else 0
                            }
            
            return trends_analysis
            
        except Exception as e:
            self.logger.error(f"Error in comprehensive trends analysis: {e}")
            return {}
    
    def _detect_seasonality_advanced(self, time_series: pd.Series) -> Dict[str, Any]:
        """ÙƒØ´Ù Ø§Ù„Ù…ÙˆØ³Ù…ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
        try:
            if len(time_series) < 14:  # Ù†Ø­ØªØ§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ©
                return {'detected': False}
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ³Ù…ÙŠØ© Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ÙŠØ© (7 Ø£ÙŠØ§Ù…)
            weekly_pattern = []
            for i in range(7):
                weekly_values = time_series.iloc[i::7]
                if len(weekly_values) > 1:
                    weekly_pattern.append(weekly_values.mean())
            
            weekly_cv = np.std(weekly_pattern) / np.mean(weekly_pattern) if len(weekly_pattern) > 0 and np.mean(weekly_pattern) != 0 else 0
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ³Ù…ÙŠØ© Ø§Ù„Ø´Ù‡Ø±ÙŠØ©
            monthly_pattern = []
            if len(time_series) >= 30:
                for i in range(30):
                    monthly_values = time_series.iloc[i::30]
                    if len(monthly_values) > 1:
                        monthly_pattern.append(monthly_values.mean())
            
            monthly_cv = np.std(monthly_pattern) / np.mean(monthly_pattern) if len(monthly_pattern) > 0 and np.mean(monthly_pattern) != 0 else 0
            
            # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…ÙˆØ³Ù…ÙŠØ©
            seasonality_detected = weekly_cv > 0.1 or monthly_cv > 0.1
            
            return {
                'detected': seasonality_detected,
                'weekly_coefficient_variation': float(weekly_cv),
                'monthly_coefficient_variation': float(monthly_cv),
                'dominant_pattern': 'weekly' if weekly_cv > monthly_cv else 'monthly' if monthly_cv > 0.1 else 'none'
            }
            
        except Exception as e:
            self.logger.error(f"Error detecting seasonality: {e}")
            return {'detected': False}
    
    async def _detect_comprehensive_anomalies(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ÙƒØ´Ù Ø´Ø§Ù…Ù„ Ù„Ù„Ø´Ø°ÙˆØ°"""
        try:
            anomalies_analysis = {}
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            
            for column in numeric_columns:
                if column in df.columns:
                    # ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±Ù‚ Ù…ØªØ¹Ø¯Ø¯Ø©
                    isolation_result = await self.anomaly_detector.detect_anomalies(df, column, "isolation_forest")
                    statistical_result = await self.anomaly_detector.detect_anomalies(df, column, "statistical")
                    
                    anomalies_analysis[column] = {
                        'isolation_forest': {
                            'anomalies_count': len(isolation_result.anomalies),
                            'anomaly_score': isolation_result.anomaly_score,
                            'confidence': isolation_result.confidence
                        },
                        'statistical': {
                            'anomalies_count': len(statistical_result.anomalies),
                            'anomaly_score': statistical_result.anomaly_score,
                            'confidence': statistical_result.confidence
                        },
                        'combined_score': (isolation_result.anomaly_score + statistical_result.anomaly_score) / 2
                    }
            
            return anomalies_analysis
            
        except Exception as e:
            self.logger.error(f"Error in comprehensive anomalies detection: {e}")
            return {}
    
    async def _generate_comprehensive_predictions(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ†Ø¨Ø¤Ø§Øª Ø´Ø§Ù…Ù„Ø©"""
        try:
            predictions_analysis = {}
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            
            # Ø§Ù„ØªÙ†Ø¨Ø¤ Ù„Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
            main_variables = ['sales', 'revenue', 'customers']
            available_variables = [col for col in main_variables if col in numeric_columns]
            
            for target_column in available_variables:
                feature_columns = [col for col in numeric_columns if col != target_column][:5]  # Ø£ÙØ¶Ù„ 5 Ù…ÙŠØ²Ø§Øª
                
                # Ø¥Ù†Ø´Ø§Ø¡ Ø·Ù„Ø¨ ØªÙ†Ø¨Ø¤
                request = PredictionRequest(
                    id=f"prediction_{target_column}",
                    name=f"ØªÙ†Ø¨Ø¤ {target_column}",
                    prediction_type=PredictionType.TIME_SERIES,
                    model_type=ModelType.RANDOM_FOREST,
                    data_source="big_data",
                    target_column=target_column,
                    feature_columns=feature_columns,
                    prediction_horizon=30  # 30 ÙØªØ±Ø© Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©
                )
                
                # Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤
                result = await self.prediction_engine.make_prediction(request, df)
                
                predictions_analysis[target_column] = {
                    'predictions_count': len(result.predictions),
                    'model_metrics': result.model_metrics,
                    'trend_analysis': result.trend_analysis,
                    'recommendations': result.recommendations[:3],  # Ø£ÙˆÙ„ 3 ØªÙˆØµÙŠØ§Øª
                    'execution_time': result.execution_time
                }
            
            return predictions_analysis
            
        except Exception as e:
            self.logger.error(f"Error in comprehensive predictions: {e}")
            return {}
    
    async def _perform_clustering_analysis(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¬Ù…ÙŠØ¹"""
        try:
            clustering_analysis = {}
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            
            if len(numeric_columns) < 2:
                return {'error': 'Ø¹Ø¯Ø¯ ØºÙŠØ± ÙƒØ§ÙÙ Ù…Ù† Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ù„Ù„ØªØ¬Ù…ÙŠØ¹'}
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¬Ù…ÙŠØ¹
            features = df[numeric_columns].fillna(df[numeric_columns].mean())
            
            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(features)
            
            # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª ÙƒØ¨ÙŠØ±
            if len(numeric_columns) > 10:
                pca = PCA(n_components=10)
                features_scaled = pca.fit_transform(features_scaled)
                clustering_analysis['dimensionality_reduction'] = {
                    'method': 'PCA',
                    'original_dimensions': len(numeric_columns),
                    'reduced_dimensions': 10,
                    'explained_variance_ratio': pca.explained_variance_ratio_.tolist()
                }
            
            # ØªØ¬Ù…ÙŠØ¹ K-Means
            optimal_k = self._find_optimal_clusters(features_scaled)
            kmeans = KMeans(n_clusters=optimal_k, random_state=42)
            kmeans_labels = kmeans.fit_predict(features_scaled)
            
            # ØªØ¬Ù…ÙŠØ¹ DBSCAN
            dbscan = DBSCAN(eps=0.5, min_samples=5)
            dbscan_labels = dbscan.fit_predict(features_scaled)
            
            clustering_analysis['kmeans'] = {
                'optimal_clusters': optimal_k,
                'cluster_sizes': np.bincount(kmeans_labels).tolist(),
                'silhouette_score': float(silhouette_score(features_scaled, kmeans_labels))
            }
            
            clustering_analysis['dbscan'] = {
                'clusters_found': len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0),
                'noise_points': np.sum(dbscan_labels == -1),
                'cluster_sizes': np.bincount(dbscan_labels[dbscan_labels != -1]).tolist() if len(dbscan_labels[dbscan_labels != -1]) > 0 else []
            }
            
            return clustering_analysis
            
        except Exception as e:
            self.logger.error(f"Error in clustering analysis: {e}")
            return {'error': str(e)}
    
    def _find_optimal_clusters(self, features: np.ndarray, max_k: int = 10) -> int:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù…Ø«Ù„ Ù„Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª"""
        try:
            if len(features) < max_k:
                max_k = len(features) - 1
            
            if max_k < 2:
                return 2
            
            silhouette_scores = []
            k_range = range(2, min(max_k + 1, 11))
            
            for k in k_range:
                kmeans = KMeans(n_clusters=k, random_state=42)
                labels = kmeans.fit_predict(features)
                score = silhouette_score(features, labels)
                silhouette_scores.append(score)
            
            optimal_k = k_range[np.argmax(silhouette_scores)]
            return optimal_k
            
        except Exception as e:
            self.logger.error(f"Error finding optimal clusters: {e}")
            return 3  # Ø§ÙØªØ±Ø§Ø¶ÙŠ

# Ù…Ø«ÙŠÙ„ Ø¹Ø§Ù… Ù„Ù…Ø­Ø±Ùƒ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø©
big_data_analytics_engine = BigDataAnalyticsEngine()

# Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø©
async def analyze_big_data_trends(data_source: str) -> Dict[str, Any]:
    """ØªØ­Ù„ÙŠÙ„ Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø©"""
    return await big_data_analytics_engine.analyze_big_data(data_source, "trends")

async def detect_big_data_anomalies(data_source: str) -> Dict[str, Any]:
    """ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø©"""
    return await big_data_analytics_engine.analyze_big_data(data_source, "anomalies")

async def predict_future_trends(data_source: str, target_variable: str, 
                              horizon: int = 30) -> Dict[str, Any]:
    """Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©"""
    try:
        # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        df = await big_data_analytics_engine._fetch_big_data(data_source)
        
        if df.empty or target_variable not in df.columns:
            return {'error': f'Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªØ§Ø­Ø© Ø£Ùˆ Ø§Ù„Ù…ØªØºÙŠØ± {target_variable} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯'}
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø·Ù„Ø¨ ØªÙ†Ø¨Ø¤
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        feature_columns = [col for col in numeric_columns if col != target_variable][:5]
        
        request = PredictionRequest(
            id=f"future_trends_{target_variable}",
            name=f"ØªÙ†Ø¨Ø¤ Ø§ØªØ¬Ø§Ù‡Ø§Øª {target_variable}",
            prediction_type=PredictionType.TIME_SERIES,
            model_type=ModelType.GRADIENT_BOOSTING,
            data_source=data_source,
            target_column=target_variable,
            feature_columns=feature_columns,
            prediction_horizon=horizon
        )
        
        # Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤
        result = await big_data_analytics_engine.prediction_engine.make_prediction(request, df)
        
        return {
            'predictions': result.predictions,
            'trend_analysis': result.trend_analysis,
            'model_metrics': result.model_metrics,
            'recommendations': result.recommendations,
            'execution_time': result.execution_time
        }
        
    except Exception as e:
        logging.error(f"Error predicting future trends: {e}")
        return {'error': str(e)}

async def comprehensive_big_data_analysis(data_source: str) -> Dict[str, Any]:
    """ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø©"""
    return await big_data_analytics_engine.analyze_big_data(data_source, "comprehensive")

if __name__ == "__main__":
    # Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
    async def main():
        # ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø©
        result = await comprehensive_big_data_analysis("sales_data")
        print(f"Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„: {json.dumps(result, indent=2, ensure_ascii=False, default=str)}")
        
        # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©
        prediction_result = await predict_future_trends("sales_data", "sales", 30)
        print(f"Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ†Ø¨Ø¤: {json.dumps(prediction_result, indent=2, ensure_ascii=False, default=str)}")
    
    asyncio.run(main())

