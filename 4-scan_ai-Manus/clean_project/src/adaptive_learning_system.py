# File: /home/ubuntu/clean_project/src/adaptive_learning_system.py
"""
Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù: /home/ubuntu/clean_project/src/adaptive_learning_system.py

Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø§Ù„ØªÙƒÙŠÙÙŠ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©
ÙŠÙˆÙØ± ØªØ¹Ù„Ù… Ù…Ø³ØªÙ…Ø± ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
"""

from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import asyncio
import json
import logging
import numpy as np
import sqlite3
from pathlib import Path
import uuid
import re
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import pickle
import yaml
from event_system import Event, EventTypes, event_bus, create_system_event

# ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

class LearningMode(Enum):
    """Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªØ¹Ù„Ù…"""
    SUPERVISED = "supervised"
    UNSUPERVISED = "unsupervised"
    REINFORCEMENT = "reinforcement"
    SEMI_SUPERVISED = "semi_supervised"
    ACTIVE_LEARNING = "active_learning"

class FeedbackType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©"""
    CORRECT = "correct"
    INCORRECT = "incorrect"
    PARTIAL = "partial"
    EXPERT_REVIEW = "expert_review"
    USER_RATING = "user_rating"

class LanguageCode(Enum):
    """Ø±Ù…ÙˆØ² Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©"""
    ARABIC = "ar"
    ENGLISH = "en"
    FRENCH = "fr"
    SPANISH = "es"

@dataclass
class LearningExample:
    """Ù…Ø«Ø§Ù„ ØªØ¹Ù„ÙŠÙ…ÙŠ"""
    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    input_data: Any = None
    expected_output: Any = None
    actual_output: Any = None
    feedback: Optional[FeedbackType] = None
    confidence: float = 0.0
    timestamp: datetime = field(default_factory=datetime.now)
    source: str = "user"
    metadata: Dict[str, Any] = field(default_factory=dict)
    weight: float = 1.0  # ÙˆØ²Ù† Ø§Ù„Ù…Ø«Ø§Ù„ ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨

@dataclass
class ModelPerformance:
    """Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
    model_id: str
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    confusion_matrix: List[List[int]]
    timestamp: datetime = field(default_factory=datetime.now)
    sample_size: int = 0
    training_time: float = 0.0

@dataclass
class NLPQuery:
    """Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©"""
    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    text: str
    language: LanguageCode = LanguageCode.ARABIC
    intent: Optional[str] = None
    entities: Dict[str, Any] = field(default_factory=dict)
    confidence: float = 0.0
    timestamp: datetime = field(default_factory=datetime.now)
    response: Optional[str] = None
    context: Dict[str, Any] = field(default_factory=dict)

class AdaptiveLearningEngine:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªÙƒÙŠÙÙŠ"""
    
    def __init__(self, model_id: str, learning_mode: LearningMode = LearningMode.SUPERVISED):
        self.model_id = model_id
        self.learning_mode = learning_mode
        self.examples: List[LearningExample] = []
        self.performance_history: List[ModelPerformance] = []
        self.model = None
        self.is_training = False
        self.last_training = None
        self.training_threshold = 100  # Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        self.performance_threshold = 0.8  # Ø­Ø¯ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
        self.logger = logging.getLogger(f'adaptive_learning_{model_id}')
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªÙƒÙŠÙÙŠ
        self.adaptation_config = {
            'auto_retrain': True,
            'retrain_interval_hours': 24,
            'min_examples_for_training': 50,
            'performance_decay_threshold': 0.05,
            'feedback_weight_multiplier': 2.0,
            'expert_feedback_weight': 5.0
        }
    
    async def add_example(self, example: LearningExample) -> bool:
        """Ø¥Ø¶Ø§ÙØ© Ù…Ø«Ø§Ù„ ØªØ¹Ù„ÙŠÙ…ÙŠ Ø¬Ø¯ÙŠØ¯"""
        try:
            # ØªØ­Ø¯ÙŠØ¯ ÙˆØ²Ù† Ø§Ù„Ù…Ø«Ø§Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©
            if example.feedback == FeedbackType.EXPERT_REVIEW:
                example.weight = self.adaptation_config['expert_feedback_weight']
            elif example.feedback in [FeedbackType.CORRECT, FeedbackType.INCORRECT]:
                example.weight = self.adaptation_config['feedback_weight_multiplier']
            
            self.examples.append(example)
            self.logger.info(f"Added learning example {example.id}")
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø§Ø¬Ø© Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            if await self._should_retrain():
                await self._schedule_retraining()
            
            # Ø¥Ø±Ø³Ø§Ù„ Ø­Ø¯Ø«
            event = create_system_event(
                EventTypes.AI_MODEL_UPDATED,
                f"New learning example added to model {self.model_id}",
                model_id=self.model_id,
                example_count=len(self.examples)
            )
            await event_bus.publish(event)
            
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to add example: {e}")
            return False
    
    async def _should_retrain(self) -> bool:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ­ØªØ§Ø¬ Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨"""
        if not self.adaptation_config['auto_retrain']:
            return False
        
        # Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        if len(self.examples) >= self.adaptation_config['min_examples_for_training']:
            return True
        
        # Ø§Ù†Ø®ÙØ§Ø¶ Ø§Ù„Ø£Ø¯Ø§Ø¡
        if self.performance_history:
            latest_performance = self.performance_history[-1]
            if len(self.performance_history) > 1:
                previous_performance = self.performance_history[-2]
                performance_decay = previous_performance.accuracy - latest_performance.accuracy
                if performance_decay > self.adaptation_config['performance_decay_threshold']:
                    return True
        
        # ÙØªØ±Ø© Ø²Ù…Ù†ÙŠØ© Ù…Ø­Ø¯Ø¯Ø©
        if self.last_training:
            hours_since_training = (datetime.now() - self.last_training).total_seconds() / 3600
            if hours_since_training >= self.adaptation_config['retrain_interval_hours']:
                return True
        
        return False
    
    async def _schedule_retraining(self):
        """Ø¬Ø¯ÙˆÙ„Ø© Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        if self.is_training:
            self.logger.info("Training already in progress, skipping")
            return
        
        self.logger.info(f"Scheduling retraining for model {self.model_id}")
        asyncio.create_task(self._retrain_model())
    
    async def _retrain_model(self):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        if self.is_training:
            return
        
        self.is_training = True
        start_time = datetime.now()
        
        try:
            self.logger.info(f"Starting retraining for model {self.model_id}")
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X, y, weights = self._prepare_training_data()
            
            if len(X) < self.adaptation_config['min_examples_for_training']:
                self.logger.warning("Not enough examples for training")
                return
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(
                X, y, weights, test_size=0.2, random_state=42, stratify=y
            )
            
            # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            if self.model is None:
                self.model = RandomForestClassifier(
                    n_estimators=100,
                    random_state=42,
                    class_weight='balanced'
                )
            
            self.model.fit(X_train, y_train, sample_weight=w_train)
            
            # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡
            y_pred = self.model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            
            # Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
            performance = ModelPerformance(
                model_id=self.model_id,
                accuracy=accuracy,
                precision=0.0,  # Ø³ÙŠØªÙ… Ø­Ø³Ø§Ø¨Ù‡Ø§ Ù„Ø§Ø­Ù‚Ø§Ù‹
                recall=0.0,
                f1_score=0.0,
                confusion_matrix=[],
                sample_size=len(X),
                training_time=(datetime.now() - start_time).total_seconds()
            )
            
            self.performance_history.append(performance)
            self.last_training = datetime.now()
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            await self._save_model()
            
            self.logger.info(f"Model retrained successfully. Accuracy: {accuracy:.3f}")
            
            # Ø¥Ø±Ø³Ø§Ù„ Ø­Ø¯Ø«
            event = create_system_event(
                EventTypes.AI_MODEL_UPDATED,
                f"Model {self.model_id} retrained successfully",
                model_id=self.model_id,
                accuracy=accuracy,
                training_time=performance.training_time
            )
            await event_bus.publish(event)
            
        except Exception as e:
            self.logger.error(f"Retraining failed: {e}")
        finally:
            self.is_training = False
    
    def _prepare_training_data(self) -> Tuple[List[Any], List[Any], List[float]]:
        """ØªØ­Ø¶ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        X, y, weights = [], [], []
        
        for example in self.examples:
            if example.expected_output is not None:
                X.append(example.input_data)
                y.append(example.expected_output)
                weights.append(example.weight)
        
        return X, y, weights
    
    async def _save_model(self):
        """Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        try:
            model_dir = Path("models/adaptive")
            model_dir.mkdir(parents=True, exist_ok=True)
            
            model_path = model_dir / f"{self.model_id}.pkl"
            with open(model_path, 'wb') as f:
                pickle.dump(self.model, f)
            
            # Ø­ÙØ¸ Ø§Ù„Ø£Ù…Ø«Ù„Ø© ÙˆØ§Ù„Ø£Ø¯Ø§Ø¡
            data = {
                'examples': [example.__dict__ for example in self.examples],
                'performance_history': [perf.__dict__ for perf in self.performance_history],
                'config': self.adaptation_config
            }
            
            data_path = model_dir / f"{self.model_id}_data.json"
            with open(data_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2, default=str)
            
            self.logger.info(f"Model saved to {model_path}")
            
        except Exception as e:
            self.logger.error(f"Failed to save model: {e}")
    
    async def load_model(self) -> bool:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        try:
            model_path = Path(f"models/adaptive/{self.model_id}.pkl")
            if model_path.exists():
                with open(model_path, 'rb') as f:
                    self.model = pickle.load(f)
                
                # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                data_path = Path(f"models/adaptive/{self.model_id}_data.json")
                if data_path.exists():
                    with open(data_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø£Ù…Ø«Ù„Ø©
                    self.examples = [
                        LearningExample(**example) for example in data.get('examples', [])
                    ]
                    
                    # Ø§Ø³ØªØ¹Ø§Ø¯Ø© ØªØ§Ø±ÙŠØ® Ø§Ù„Ø£Ø¯Ø§Ø¡
                    self.performance_history = [
                        ModelPerformance(**perf) for perf in data.get('performance_history', [])
                    ]
                    
                    # Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
                    self.adaptation_config.update(data.get('config', {}))
                
                self.logger.info(f"Model loaded from {model_path}")
                return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"Failed to load model: {e}")
            return False
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        if not self.performance_history:
            return {}
        
        latest = self.performance_history[-1]
        return {
            'current_accuracy': latest.accuracy,
            'training_examples': len(self.examples),
            'last_training': self.last_training,
            'performance_trend': self._calculate_performance_trend(),
            'model_age_hours': (datetime.now() - latest.timestamp).total_seconds() / 3600
        }
    
    def _calculate_performance_trend(self) -> str:
        """Ø­Ø³Ø§Ø¨ Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        if len(self.performance_history) < 2:
            return "insufficient_data"
        
        recent_performances = [p.accuracy for p in self.performance_history[-5:]]
        if len(recent_performances) < 2:
            return "insufficient_data"
        
        trend = np.polyfit(range(len(recent_performances)), recent_performances, 1)[0]
        
        if trend > 0.01:
            return "improving"
        elif trend < -0.01:
            return "declining"
        else:
            return "stable"

class NLPProcessor:
    """Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©"""
    
    def __init__(self):
        self.supported_languages = [lang.value for lang in LanguageCode]
        self.stemmer = SnowballStemmer('arabic')
        self.stop_words = set(stopwords.words('arabic'))
        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words=list(self.stop_words))
        self.intent_classifier = None
        self.entity_patterns = {}
        self.knowledge_base = {}
        self.logger = logging.getLogger('nlp_processor')
        
        # ØªØ­Ù…ÙŠÙ„ Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª
        self._load_entity_patterns()
        
        # ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
        self._load_knowledge_base()
    
    def _load_entity_patterns(self):
        """ØªØ­Ù…ÙŠÙ„ Ù‚ÙˆØ§Ù„Ø¨ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª"""
        self.entity_patterns = {
            'crop_name': [
                r'\b(Ø·Ù…Ø§Ø·Ù…|Ø¨Ù†Ø¯ÙˆØ±Ø©|Ø®ÙŠØ§Ø±|ÙÙ„ÙÙ„|Ø¨Ø§Ø°Ù†Ø¬Ø§Ù†|ÙƒÙˆØ³Ø§|Ø¨Ø·Ø§Ø·Ø³|Ø¬Ø²Ø±|Ø¨ØµÙ„|Ø«ÙˆÙ…)\b',
                r'\b(Ù‚Ù…Ø­|Ø´Ø¹ÙŠØ±|Ø°Ø±Ø©|Ø£Ø±Ø²|Ø¹Ø¯Ø³|Ø­Ù…Øµ|ÙÙˆÙ„|Ù„ÙˆØ¨ÙŠØ§)\b',
                r'\b(ØªÙØ§Ø­|Ø¨Ø±ØªÙ‚Ø§Ù„|Ù„ÙŠÙ…ÙˆÙ†|Ø¹Ù†Ø¨|Ù…ÙˆØ²|Ù…Ø§Ù†Ø¬Ùˆ|Ø®ÙˆØ®|Ù…Ø´Ù…Ø´)\b'
            ],
            'disease_name': [
                r'\b(ØªØ¨Ù‚Ø¹ Ø§Ù„Ø£ÙˆØ±Ø§Ù‚|Ø§Ù„ØµØ¯Ø£|Ø§Ù„Ø¹ÙÙ† Ø§Ù„Ø±Ù…Ø§Ø¯ÙŠ|Ø§Ù„Ø¨ÙŠØ§Ø¶ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ÙŠ|ØªØ¹ÙÙ† Ø§Ù„Ø¬Ø°ÙˆØ±)\b',
                r'\b(Ø§Ù„Ø°Ø¨ÙˆÙ„ Ø§Ù„Ø¨ÙƒØªÙŠØ±ÙŠ|ÙÙŠØ±ÙˆØ³ ØªØ¬Ø¹Ø¯ Ø§Ù„Ø£ÙˆØ±Ø§Ù‚|Ø§Ù„Ø¹ÙÙ† Ø§Ù„Ø£Ø³ÙˆØ¯|Ø­Ø±ÙˆÙ‚ Ø§Ù„Ø´Ù…Ø³)\b'
            ],
            'symptom': [
                r'\b(Ø§ØµÙØ±Ø§Ø±|Ø°Ø¨ÙˆÙ„|ØªØ¨Ù‚Ø¹|ØªØ¬Ø¹Ø¯|ØªØ¹ÙÙ†|Ø¬ÙØ§Ù|Ø³Ù‚ÙˆØ·)\b.*\b(Ø£ÙˆØ±Ø§Ù‚|Ø«Ù…Ø§Ø±|Ø¬Ø°ÙˆØ±|Ø³Ø§Ù‚)\b',
                r'\b(Ø¨Ù‚Ø¹|Ù„Ø·Ø®|Ø®Ø·ÙˆØ·|Ø«Ù‚ÙˆØ¨|ØªØ´Ù‚Ù‚Ø§Øª)\b.*\b(Ø¨Ù†ÙŠØ©|ØµÙØ±Ø§Ø¡|Ø³ÙˆØ¯Ø§Ø¡|Ø¨ÙŠØ¶Ø§Ø¡)\b'
            ],
            'location': [
                r'\b(Ø­Ù‚Ù„|Ù…Ø²Ø±Ø¹Ø©|Ø¨ÙŠØª Ù…Ø­Ù…ÙŠ|ØµÙˆØ¨Ø©|Ø­Ø¯ÙŠÙ‚Ø©|Ø£Ø±Ø¶)\b',
                r'\b(Ø´Ù…Ø§Ù„|Ø¬Ù†ÙˆØ¨|Ø´Ø±Ù‚|ØºØ±Ø¨|ÙˆØ³Ø·)\b.*\b(Ø§Ù„Ù…Ù…Ù„ÙƒØ©|Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©|Ø§Ù„Ø±ÙŠØ§Ø¶|Ø¬Ø¯Ø©|Ø§Ù„Ø¯Ù…Ø§Ù…)\b'
            ],
            'time': [
                r'\b(Ø§Ù„ÙŠÙˆÙ…|Ø£Ù…Ø³|ØºØ¯Ø§Ù‹|Ù‡Ø°Ø§ Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹|Ø§Ù„Ø´Ù‡Ø± Ø§Ù„Ù…Ø§Ø¶ÙŠ|Ø§Ù„Ø¹Ø§Ù… Ø§Ù„Ù…Ø§Ø¶ÙŠ)\b',
                r'\b(ØµØ¨Ø§Ø­|Ù…Ø³Ø§Ø¡|Ø¸Ù‡Ø±|Ù„ÙŠÙ„)\b',
                r'\b(ÙŠÙ†Ø§ÙŠØ±|ÙØ¨Ø±Ø§ÙŠØ±|Ù…Ø§Ø±Ø³|Ø£Ø¨Ø±ÙŠÙ„|Ù…Ø§ÙŠÙˆ|ÙŠÙˆÙ†ÙŠÙˆ|ÙŠÙˆÙ„ÙŠÙˆ|Ø£ØºØ³Ø·Ø³|Ø³Ø¨ØªÙ…Ø¨Ø±|Ø£ÙƒØªÙˆØ¨Ø±|Ù†ÙˆÙÙ…Ø¨Ø±|Ø¯ÙŠØ³Ù…Ø¨Ø±)\b'
            ]
        }
    
    def _load_knowledge_base(self):
        """ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©"""
        self.knowledge_base = {
            'diseases': {
                'ØªØ¨Ù‚Ø¹ Ø§Ù„Ø£ÙˆØ±Ø§Ù‚': {
                    'description': 'Ù…Ø±Ø¶ ÙØ·Ø±ÙŠ ÙŠØµÙŠØ¨ Ø£ÙˆØ±Ø§Ù‚ Ø§Ù„Ù†Ø¨Ø§ØªØ§Øª ÙˆÙŠØ³Ø¨Ø¨ Ø¸Ù‡ÙˆØ± Ø¨Ù‚Ø¹ Ø¨Ù†ÙŠØ© Ø£Ùˆ Ø³ÙˆØ¯Ø§Ø¡',
                    'symptoms': ['Ø¨Ù‚Ø¹ Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø£ÙˆØ±Ø§Ù‚', 'Ø§ØµÙØ±Ø§Ø± Ø§Ù„Ø£ÙˆØ±Ø§Ù‚', 'Ø³Ù‚ÙˆØ· Ø§Ù„Ø£ÙˆØ±Ø§Ù‚ Ø§Ù„Ù…Ø¨ÙƒØ±'],
                    'causes': ['Ø§Ù„Ø±Ø·ÙˆØ¨Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ©', 'Ø³ÙˆØ¡ Ø§Ù„ØªÙ‡ÙˆÙŠØ©', 'Ø§Ù„Ø±ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø£ÙˆØ±Ø§Ù‚'],
                    'treatment': ['Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¨ÙŠØ¯Ø§Øª ÙØ·Ø±ÙŠØ©', 'ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙ‡ÙˆÙŠØ©', 'ØªØ¬Ù†Ø¨ Ø§Ù„Ø±ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø£ÙˆØ±Ø§Ù‚'],
                    'prevention': ['Ø²Ø±Ø§Ø¹Ø© Ø£ØµÙ†Ø§Ù Ù…Ù‚Ø§ÙˆÙ…Ø©', 'ØªØ·Ø¨ÙŠÙ‚ Ø¯ÙˆØ±Ø© Ø²Ø±Ø§Ø¹ÙŠØ©', 'ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø­Ù‚Ù„']
                },
                'Ø§Ù„ØµØ¯Ø£': {
                    'description': 'Ù…Ø±Ø¶ ÙØ·Ø±ÙŠ ÙŠØ³Ø¨Ø¨ Ø¸Ù‡ÙˆØ± Ø¨Ù‚Ø¹ ØµØ¯Ø¦ÙŠØ© Ø§Ù„Ù„ÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£ÙˆØ±Ø§Ù‚',
                    'symptoms': ['Ø¨Ù‚Ø¹ Ø¨Ø±ØªÙ‚Ø§Ù„ÙŠØ© Ø£Ùˆ Ø¨Ù†ÙŠØ©', 'Ù…Ø³Ø­ÙˆÙ‚ ØµØ¯Ø¦ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø£ÙˆØ±Ø§Ù‚'],
                    'causes': ['Ø§Ù„Ø±Ø·ÙˆØ¨Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ©', 'Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø­Ø±Ø§Ø±Ø© Ø§Ù„Ù…Ø¹ØªØ¯Ù„Ø©'],
                    'treatment': ['Ù…Ø¨ÙŠØ¯Ø§Øª ÙØ·Ø±ÙŠØ© Ø¬Ù‡Ø§Ø²ÙŠØ©', 'Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„Ù…ØµØ§Ø¨Ø©'],
                    'prevention': ['Ø²Ø±Ø§Ø¹Ø© Ø£ØµÙ†Ø§Ù Ù…Ù‚Ø§ÙˆÙ…Ø©', 'ØªØ­Ø³ÙŠÙ† Ø¯ÙˆØ±Ø§Ù† Ø§Ù„Ù‡ÙˆØ§Ø¡']
                }
            },
            'crops': {
                'Ø·Ù…Ø§Ø·Ù…': {
                    'optimal_conditions': {
                        'temperature': '20-25Â°C',
                        'humidity': '60-70%',
                        'ph': '6.0-6.8'
                    },
                    'common_diseases': ['ØªØ¨Ù‚Ø¹ Ø§Ù„Ø£ÙˆØ±Ø§Ù‚', 'Ø§Ù„Ø°Ø¨ÙˆÙ„ Ø§Ù„Ø¨ÙƒØªÙŠØ±ÙŠ', 'Ø§Ù„Ø¹ÙÙ† Ø§Ù„Ø±Ù…Ø§Ø¯ÙŠ'],
                    'planting_season': 'Ø§Ù„Ø±Ø¨ÙŠØ¹ ÙˆØ§Ù„Ø®Ø±ÙŠÙ',
                    'harvest_time': '75-85 ÙŠÙˆÙ… Ù…Ù† Ø§Ù„Ø²Ø±Ø§Ø¹Ø©'
                },
                'Ø®ÙŠØ§Ø±': {
                    'optimal_conditions': {
                        'temperature': '18-24Â°C',
                        'humidity': '70-80%',
                        'ph': '6.0-7.0'
                    },
                    'common_diseases': ['Ø§Ù„Ø¨ÙŠØ§Ø¶ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ÙŠ', 'Ø§Ù„Ø¹ÙÙ† Ø§Ù„Ø±Ù…Ø§Ø¯ÙŠ'],
                    'planting_season': 'Ø§Ù„Ø±Ø¨ÙŠØ¹ ÙˆØ§Ù„ØµÙŠÙ',
                    'harvest_time': '50-60 ÙŠÙˆÙ… Ù…Ù† Ø§Ù„Ø²Ø±Ø§Ø¹Ø©'
                }
            },
            'general_advice': {
                'irrigation': 'Ø§Ø³Ù‚ Ø§Ù„Ù†Ø¨Ø§ØªØ§Øª ÙÙŠ Ø§Ù„ØµØ¨Ø§Ø­ Ø§Ù„Ø¨Ø§ÙƒØ± Ø£Ùˆ Ø§Ù„Ù…Ø³Ø§Ø¡ Ù„ØªØ¬Ù†Ø¨ ØªØ¨Ø®Ø± Ø§Ù„Ù…Ø§Ø¡',
                'fertilization': 'Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø³Ù…Ø¯Ø© Ø§Ù„Ø¹Ø¶ÙˆÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ±Ø¨Ø©',
                'pest_control': 'Ø±Ø§Ù‚Ø¨ Ø§Ù„Ù†Ø¨Ø§ØªØ§Øª Ø¨Ø§Ù†ØªØ¸Ø§Ù… Ù„Ù„ÙƒØ´Ù Ø§Ù„Ù…Ø¨ÙƒØ± Ø¹Ù† Ø§Ù„Ø¢ÙØ§Øª',
                'soil_preparation': 'Ø§Ø­Ø±Ø« Ø§Ù„ØªØ±Ø¨Ø© Ø¬ÙŠØ¯Ø§Ù‹ ÙˆØ£Ø¶Ù Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ø¹Ø¶ÙˆÙŠØ© Ù‚Ø¨Ù„ Ø§Ù„Ø²Ø±Ø§Ø¹Ø©'
            }
        }
    
    async def process_query(self, query: NLPQuery) -> NLPQuery:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©"""
        try:
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
            cleaned_text = self._clean_text(query.text)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª
            entities = self._extract_entities(cleaned_text)
            query.entities = entities
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†ÙŠØ©
            intent = await self._classify_intent(cleaned_text, entities)
            query.intent = intent
            
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
            response = await self._generate_response(query)
            query.response = response
            
            self.logger.info(f"Processed NLP query: {query.id}")
            return query
            
        except Exception as e:
            self.logger.error(f"Failed to process query: {e}")
            query.response = "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø³ØªÙØ³Ø§Ø±Ùƒ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."
            return query
    
    def _clean_text(self, text: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ"""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©
        text = re.sub(r'[^\w\s\u0600-\u06FF]', ' ', text)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def _extract_entities(self, text: str) -> Dict[str, List[str]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù†Øµ"""
        entities = {}
        
        for entity_type, patterns in self.entity_patterns.items():
            matches = []
            for pattern in patterns:
                found = re.findall(pattern, text, re.IGNORECASE)
                matches.extend(found)
            
            if matches:
                entities[entity_type] = list(set(matches))  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±
        
        return entities
    
    async def _classify_intent(self, text: str, entities: Dict[str, List[str]]) -> str:
        """ØªØµÙ†ÙŠÙ Ù†ÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        # Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ù†ÙˆØ§ÙŠØ§
        intent_patterns = {
            'disease_diagnosis': [
                r'\b(Ù…Ø§ Ù‡Ùˆ|Ù…Ø§ Ù‡Ø°Ø§|ØªØ´Ø®ÙŠØµ|Ù…Ø±Ø¶|Ù…Ø´ÙƒÙ„Ø©|Ø¹Ù„Ø©)\b.*\b(Ù†Ø¨Ø§Øª|Ù…Ø­ØµÙˆÙ„|Ø²Ø±Ø¹)\b',
                r'\b(Ø£ÙˆØ±Ø§Ù‚|Ø«Ù…Ø§Ø±|Ø¬Ø°ÙˆØ±)\b.*\b(Ù…Ø±ÙŠØ¶Ø©|Ù…ØµØ§Ø¨Ø©|ØªØ§Ù„ÙØ©|Ø°Ø§Ø¨Ù„Ø©)\b'
            ],
            'treatment_advice': [
                r'\b(ÙƒÙŠÙ|Ù…Ø§Ø°Ø§)\b.*\b(Ø¹Ù„Ø§Ø¬|Ù…Ø¹Ø§Ù„Ø¬Ø©|Ø­Ù„|Ø¥ØµÙ„Ø§Ø­)\b',
                r'\b(Ø¹Ù„Ø§Ø¬|Ø¯ÙˆØ§Ø¡|Ù…Ø¨ÙŠØ¯|Ø±Ø´)\b.*\b(Ù…Ø±Ø¶|Ø¢ÙØ©|Ø­Ø´Ø±Ø©)\b'
            ],
            'prevention_tips': [
                r'\b(ÙƒÙŠÙ|Ù…Ø§Ø°Ø§)\b.*\b(ÙˆÙ‚Ø§ÙŠØ©|Ù…Ù†Ø¹|ØªØ¬Ù†Ø¨|Ø­Ù…Ø§ÙŠØ©)\b',
                r'\b(ÙˆÙ‚Ø§ÙŠØ©|Ù…Ù†Ø¹|Ø­Ù…Ø§ÙŠØ©)\b.*\b(Ù…Ø±Ø¶|Ø¢ÙØ©|Ù…Ø´ÙƒÙ„Ø©)\b'
            ],
            'crop_information': [
                r'\b(Ù…Ø¹Ù„ÙˆÙ…Ø§Øª|ØªÙØ§ØµÙŠÙ„|Ø®ØµØ§Ø¦Øµ)\b.*\b(Ù…Ø­ØµÙˆÙ„|Ù†Ø¨Ø§Øª|Ø²Ø±Ø§Ø¹Ø©)\b',
                r'\b(Ù…ØªÙ‰|ÙƒÙŠÙ|Ø£ÙŠÙ†)\b.*\b(Ø²Ø±Ø§Ø¹Ø©|ØºØ±Ø³|Ø¨Ø°Ø±)\b'
            ],
            'general_question': [
                r'\b(Ù…Ø§ Ù‡Ùˆ|Ù…Ø§ Ù‡ÙŠ|ÙƒÙŠÙ|Ù…ØªÙ‰|Ø£ÙŠÙ†|Ù„Ù…Ø§Ø°Ø§|Ù…Ø§Ø°Ø§)\b',
                r'\b(Ø³Ø¤Ø§Ù„|Ø§Ø³ØªÙØ³Ø§Ø±|Ù…Ø¹Ù„ÙˆÙ…Ø©|Ù†ØµÙŠØ­Ø©)\b'
            ]
        }
        
        # ØªØ³Ø¬ÙŠÙ„ Ù†Ù‚Ø§Ø· Ù„ÙƒÙ„ Ù†ÙŠØ©
        intent_scores = {}
        
        for intent, patterns in intent_patterns.items():
            score = 0
            for pattern in patterns:
                if re.search(pattern, text, re.IGNORECASE):
                    score += 1
            
            # Ø¥Ø¶Ø§ÙØ© Ù†Ù‚Ø§Ø· Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
            if intent == 'disease_diagnosis' and 'disease_name' in entities:
                score += 2
            elif intent == 'crop_information' and 'crop_name' in entities:
                score += 2
            elif intent == 'treatment_advice' and ('disease_name' in entities or 'symptom' in entities):
                score += 2
            
            intent_scores[intent] = score
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†ÙŠØ© Ø°Ø§Øª Ø£Ø¹Ù„Ù‰ Ù†Ù‚Ø§Ø·
        if intent_scores:
            best_intent = max(intent_scores, key=intent_scores.get)
            if intent_scores[best_intent] > 0:
                return best_intent
        
        return 'general_question'
    
    async def _generate_response(self, query: NLPQuery) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©"""
        intent = query.intent
        entities = query.entities
        text = query.text
        
        if intent == 'disease_diagnosis':
            return self._generate_diagnosis_response(entities, text)
        elif intent == 'treatment_advice':
            return self._generate_treatment_response(entities, text)
        elif intent == 'prevention_tips':
            return self._generate_prevention_response(entities, text)
        elif intent == 'crop_information':
            return self._generate_crop_info_response(entities, text)
        else:
            return self._generate_general_response(text)
    
    def _generate_diagnosis_response(self, entities: Dict[str, List[str]], text: str) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø© Ù„Ù„ØªØ´Ø®ÙŠØµ"""
        if 'disease_name' in entities:
            disease = entities['disease_name'][0]
            if disease in self.knowledge_base['diseases']:
                disease_info = self.knowledge_base['diseases'][disease]
                return f"""
Ù…Ø±Ø¶ {disease}:

Ø§Ù„ÙˆØµÙ: {disease_info['description']}

Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶:
{chr(10).join(f"â€¢ {symptom}" for symptom in disease_info['symptoms'])}

Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨:
{chr(10).join(f"â€¢ {cause}" for cause in disease_info['causes'])}

Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ´Ø®ÙŠØµ Ø¯Ù‚ÙŠÙ‚ØŒ ÙŠØ±Ø¬Ù‰ Ø±ÙØ¹ ØµÙˆØ±Ø© ÙˆØ§Ø¶Ø­Ø© Ù„Ù„Ù†Ø¨Ø§Øª Ø§Ù„Ù…ØµØ§Ø¨.
                """.strip()
        
        if 'symptom' in entities:
            return """
Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø©ØŒ Ø£Ù†ØµØ­Ùƒ Ø¨Ù€:

1. Ø±ÙØ¹ ØµÙˆØ±Ø© ÙˆØ§Ø¶Ø­Ø© Ù„Ù„Ù†Ø¨Ø§Øª Ø§Ù„Ù…ØµØ§Ø¨ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ´Ø®ÙŠØµ Ø¯Ù‚ÙŠÙ‚
2. ÙØ­Øµ Ø§Ù„Ù†Ø¨Ø§ØªØ§Øª Ø§Ù„Ù…Ø¬Ø§ÙˆØ±Ø© Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ù…Ø´ÙƒÙ„Ø©
3. ØªØ¬Ù†Ø¨ Ø§Ù„Ø±ÙŠ Ø§Ù„Ù…ÙØ±Ø· Ø£Ùˆ Ù†Ù‚Øµ Ø§Ù„Ø±ÙŠ
4. Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„ØªÙ‡ÙˆÙŠØ© Ø­ÙˆÙ„ Ø§Ù„Ù†Ø¨Ø§ØªØ§Øª

ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„ Ø¥Ø°Ø§ Ù‚Ù…Øª Ø¨Ø±ÙØ¹ ØµÙˆØ±Ø© Ù„Ù„Ù†Ø¨Ø§Øª.
            """.strip()
        
        return """
Ù„ØªØ´Ø®ÙŠØµ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ù†Ø¨Ø§Øª Ø¨Ø¯Ù‚Ø©ØŒ Ø£Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰:

1. ØµÙˆØ±Ø© ÙˆØ§Ø¶Ø­Ø© Ù„Ù„Ù†Ø¨Ø§Øª Ø§Ù„Ù…ØµØ§Ø¨
2. ÙˆØµÙ Ù…ÙØµÙ„ Ù„Ù„Ø£Ø¹Ø±Ø§Ø¶
3. Ù†ÙˆØ¹ Ø§Ù„Ù†Ø¨Ø§Øª Ø£Ùˆ Ø§Ù„Ù…Ø­ØµÙˆÙ„
4. Ø§Ù„Ø¸Ø±ÙˆÙ Ø§Ù„Ø¨ÙŠØ¦ÙŠØ© (Ø§Ù„Ø±ÙŠØŒ Ø§Ù„ØªØ³Ù…ÙŠØ¯ØŒ Ø§Ù„Ø·Ù‚Ø³)

ÙŠØ±Ø¬Ù‰ Ø±ÙØ¹ ØµÙˆØ±Ø© ÙˆØ³Ø£Ù‚ÙˆÙ… Ø¨ØªØ­Ù„ÙŠÙ„Ù‡Ø§ ÙÙˆØ±Ø§Ù‹.
        """.strip()
    
    def _generate_treatment_response(self, entities: Dict[str, List[str]], text: str) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø© Ù„Ù„Ø¹Ù„Ø§Ø¬"""
        if 'disease_name' in entities:
            disease = entities['disease_name'][0]
            if disease in self.knowledge_base['diseases']:
                treatments = self.knowledge_base['diseases'][disease]['treatment']
                return f"""
Ø¹Ù„Ø§Ø¬ {disease}:

{chr(10).join(f"{i+1}. {treatment}" for i, treatment in enumerate(treatments))}

Ù†ØµØ§Ø¦Ø­ Ø¥Ø¶Ø§ÙÙŠØ©:
â€¢ Ø§ØªØ¨Ø¹ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¨ÙŠØ¯Ø§Øª Ø¨Ø¯Ù‚Ø©
â€¢ ÙƒØ±Ø± Ø§Ù„Ø¹Ù„Ø§Ø¬ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©
â€¢ Ø±Ø§Ù‚Ø¨ ØªØ­Ø³Ù† Ø§Ù„Ø­Ø§Ù„Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ø¹Ù„Ø§Ø¬
â€¢ Ø§Ø³ØªØ´Ø± Ø®Ø¨ÙŠØ± Ø²Ø±Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø´Ø¯ÙŠØ¯Ø©
                """.strip()
        
        return """
Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØµØ§Ø¦Ø­ Ø¹Ù„Ø§Ø¬ÙŠØ© Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ Ø£Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…Ø¹Ø±ÙØ©:

1. Ù†ÙˆØ¹ Ø§Ù„Ù…Ø±Ø¶ Ø£Ùˆ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©
2. Ù†ÙˆØ¹ Ø§Ù„Ù†Ø¨Ø§Øª Ø§Ù„Ù…ØµØ§Ø¨
3. Ø´Ø¯Ø© Ø§Ù„Ø¥ØµØ§Ø¨Ø©
4. Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ Ø§Ù„Ø¸Ø§Ù‡Ø±Ø©

ÙŠØ±Ø¬Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø£Ùˆ Ø±ÙØ¹ ØµÙˆØ±Ø© Ù„Ù„Ù†Ø¨Ø§Øª.
        """.strip()
    
    def _generate_prevention_response(self, entities: Dict[str, List[str]], text: str) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø© Ù„Ù„ÙˆÙ‚Ø§ÙŠØ©"""
        if 'disease_name' in entities:
            disease = entities['disease_name'][0]
            if disease in self.knowledge_base['diseases']:
                prevention = self.knowledge_base['diseases'][disease]['prevention']
                return f"""
Ø§Ù„ÙˆÙ‚Ø§ÙŠØ© Ù…Ù† {disease}:

{chr(10).join(f"â€¢ {tip}" for tip in prevention)}

Ù†ØµØ§Ø¦Ø­ Ø¹Ø§Ù…Ø© Ù„Ù„ÙˆÙ‚Ø§ÙŠØ©:
â€¢ ÙØ­Øµ Ø§Ù„Ù†Ø¨Ø§ØªØ§Øª Ø¨Ø§Ù†ØªØ¸Ø§Ù…
â€¢ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ù†Ø¸Ø§ÙØ© Ø§Ù„Ø­Ù‚Ù„
â€¢ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø°ÙˆØ± Ù…Ø¹ØªÙ…Ø¯Ø©
â€¢ ØªØ·Ø¨ÙŠÙ‚ Ù…Ù…Ø§Ø±Ø³Ø§Øª Ø²Ø±Ø§Ø¹ÙŠØ© Ø¬ÙŠØ¯Ø©
                """.strip()
        
        return """
Ù†ØµØ§Ø¦Ø­ Ø¹Ø§Ù…Ø© Ù„Ù„ÙˆÙ‚Ø§ÙŠØ© Ù…Ù† Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ù†Ø¨Ø§ØªØ§Øª:

1. Ø§Ø®ØªÙŠØ§Ø± Ø£ØµÙ†Ø§Ù Ù…Ù‚Ø§ÙˆÙ…Ø© Ù„Ù„Ø£Ù…Ø±Ø§Ø¶
2. ØªØ·Ø¨ÙŠÙ‚ Ø¯ÙˆØ±Ø© Ø²Ø±Ø§Ø¹ÙŠØ© Ù…Ù†Ø§Ø³Ø¨Ø©
3. ØªØ­Ø³ÙŠÙ† ØªØµØ±ÙŠÙ Ø§Ù„ØªØ±Ø¨Ø© ÙˆØ§Ù„ØªÙ‡ÙˆÙŠØ©
4. ØªØ¬Ù†Ø¨ Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„Ø±ÙŠ ÙˆØ§Ù„ØªØ³Ù…ÙŠØ¯
5. Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø®Ù„ÙØ§Øª Ø§Ù„Ù†Ø¨Ø§ØªÙŠØ© Ø§Ù„Ù…ØµØ§Ø¨Ø©
6. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¨ÙŠØ¯Ø§Øª ÙˆÙ‚Ø§Ø¦ÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
7. Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù†Ø¨Ø§ØªØ§Øª Ø¨Ø§Ù†ØªØ¸Ø§Ù…

Ø§Ù„ÙˆÙ‚Ø§ÙŠØ© Ø®ÙŠØ± Ù…Ù† Ø§Ù„Ø¹Ù„Ø§Ø¬!
        """.strip()
    
    def _generate_crop_info_response(self, entities: Dict[str, List[str]], text: str) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø© Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø­Ø§ØµÙŠÙ„"""
        if 'crop_name' in entities:
            crop = entities['crop_name'][0]
            if crop in self.knowledge_base['crops']:
                crop_info = self.knowledge_base['crops'][crop]
                return f"""
Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø²Ø±Ø§Ø¹Ø© {crop}:

Ø§Ù„Ø¸Ø±ÙˆÙ Ø§Ù„Ù…Ø«Ù„Ù‰:
â€¢ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø©: {crop_info['optimal_conditions']['temperature']}
â€¢ Ø§Ù„Ø±Ø·ÙˆØ¨Ø©: {crop_info['optimal_conditions']['humidity']}
â€¢ Ø­Ù…ÙˆØ¶Ø© Ø§Ù„ØªØ±Ø¨Ø©: {crop_info['optimal_conditions']['ph']}

Ù…ÙˆØ³Ù… Ø§Ù„Ø²Ø±Ø§Ø¹Ø©: {crop_info['planting_season']}
ÙˆÙ‚Øª Ø§Ù„Ø­ØµØ§Ø¯: {crop_info['harvest_time']}

Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©:
{chr(10).join(f"â€¢ {disease}" for disease in crop_info['common_diseases'])}
                """.strip()
        
        return """
ÙŠØ±Ø¬Ù‰ ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØµÙˆÙ„ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙØµÙ„Ø© Ø¹Ù†:

â€¢ Ø§Ù„Ø¸Ø±ÙˆÙ Ø§Ù„Ù…Ø«Ù„Ù‰ Ù„Ù„Ø²Ø±Ø§Ø¹Ø©
â€¢ Ù…ÙˆØ§Ø³Ù… Ø§Ù„Ø²Ø±Ø§Ø¹Ø© ÙˆØ§Ù„Ø­ØµØ§Ø¯
â€¢ Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ ÙˆØ§Ù„Ø¢ÙØ§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
â€¢ Ù†ØµØ§Ø¦Ø­ Ø§Ù„Ø±Ø¹Ø§ÙŠØ© ÙˆØ§Ù„ØµÙŠØ§Ù†Ø©
â€¢ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„ØªØ±Ø¨Ø© ÙˆØ§Ù„Ø±ÙŠ
        """.strip()
    
    def _generate_general_response(self, text: str) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ø§Ù…Ø©"""
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¹Ø§Ù…Ø©
        for topic, advice in self.knowledge_base['general_advice'].items():
            if any(keyword in text for keyword in [topic, advice.split()[0]]):
                return f"Ù†ØµÙŠØ­Ø© Ø­ÙˆÙ„ {topic}:\n\n{advice}"
        
        return """
Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ù†Ø¸Ø§Ù… Gaara Scan AI Ù„Ù„Ø²Ø±Ø§Ø¹Ø© Ø§Ù„Ø°ÙƒÙŠØ©!

ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ:

ğŸ” ØªØ´Ø®ÙŠØµ Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ù†Ø¨Ø§ØªØ§Øª (Ø§Ø±ÙØ¹ ØµÙˆØ±Ø©)
ğŸ’Š Ù†ØµØ§Ø¦Ø­ Ø§Ù„Ø¹Ù„Ø§Ø¬ ÙˆØ§Ù„ÙˆÙ‚Ø§ÙŠØ©
ğŸŒ± Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø§Ù„Ù…Ø­Ø§ØµÙŠÙ„ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø²Ø±Ø¹Ø©
ğŸ¤– Ø§Ø³ØªØ´Ø§Ø±Ø§Øª Ø²Ø±Ø§Ø¹ÙŠØ© Ø°ÙƒÙŠØ©

ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ
        """.strip()

class AdaptiveLearningManager:
    """Ù…Ø¯ÙŠØ± Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªÙƒÙŠÙÙŠ"""
    
    def __init__(self):
        self.learning_engines: Dict[str, AdaptiveLearningEngine] = {}
        self.nlp_processor = NLPProcessor()
        self.feedback_queue = asyncio.Queue()
        self.logger = logging.getLogger('adaptive_learning_manager')
    
    def create_learning_engine(self, model_id: str, learning_mode: LearningMode = LearningMode.SUPERVISED) -> AdaptiveLearningEngine:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ø±Ùƒ ØªØ¹Ù„Ù… Ø¬Ø¯ÙŠØ¯"""
        engine = AdaptiveLearningEngine(model_id, learning_mode)
        self.learning_engines[model_id] = engine
        self.logger.info(f"Created learning engine for model {model_id}")
        return engine
    
    async def process_feedback(self, model_id: str, input_data: Any, expected_output: Any, 
                             actual_output: Any, feedback_type: FeedbackType, 
                             confidence: float = 0.0, metadata: Dict[str, Any] = None) -> bool:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©"""
        if model_id not in self.learning_engines:
            self.logger.warning(f"Learning engine not found for model {model_id}")
            return False
        
        example = LearningExample(
            input_data=input_data,
            expected_output=expected_output,
            actual_output=actual_output,
            feedback=feedback_type,
            confidence=confidence,
            metadata=metadata or {}
        )
        
        engine = self.learning_engines[model_id]
        return await engine.add_example(example)
    
    async def process_nlp_query(self, text: str, language: LanguageCode = LanguageCode.ARABIC, 
                               context: Dict[str, Any] = None) -> NLPQuery:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©"""
        query = NLPQuery(
            text=text,
            language=language,
            context=context or {}
        )
        
        return await self.nlp_processor.process_query(query)
    
    def get_all_performance_metrics(self) -> Dict[str, Dict[str, Any]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª"""
        metrics = {}
        for model_id, engine in self.learning_engines.items():
            metrics[model_id] = engine.get_performance_metrics()
        return metrics
    
    async def initialize_all_engines(self) -> bool:
        """ØªÙ‡ÙŠØ¦Ø© Ø¬Ù…ÙŠØ¹ Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„ØªØ¹Ù„Ù…"""
        try:
            for engine in self.learning_engines.values():
                await engine.load_model()
            
            self.logger.info(f"Initialized {len(self.learning_engines)} learning engines")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to initialize learning engines: {e}")
            return False

# Ù…Ø«ÙŠÙ„ Ø¹Ø§Ù… Ù„Ù…Ø¯ÙŠØ± Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªÙƒÙŠÙÙŠ
adaptive_learning_manager = AdaptiveLearningManager()

# Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø©
async def initialize_adaptive_learning():
    """ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªÙƒÙŠÙÙŠ"""
    try:
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
        adaptive_learning_manager.create_learning_engine("disease_diagnosis", LearningMode.SUPERVISED)
        adaptive_learning_manager.create_learning_engine("crop_recommendation", LearningMode.SUPERVISED)
        adaptive_learning_manager.create_learning_engine("yield_prediction", LearningMode.SUPERVISED)
        
        # ØªÙ‡ÙŠØ¦Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª
        await adaptive_learning_manager.initialize_all_engines()
        
        # Ø¥Ø±Ø³Ø§Ù„ Ø­Ø¯Ø«
        event = create_system_event(
            EventTypes.AI_MODEL_LOADED,
            "Adaptive learning system initialized"
        )
        await event_bus.publish(event)
        
        return True
        
    except Exception as e:
        logging.error(f"Failed to initialize adaptive learning: {e}")
        return False

async def submit_feedback(model_id: str, input_data: Any, expected_output: Any, 
                         actual_output: Any, feedback_type: str, confidence: float = 0.0) -> bool:
    """ØªÙ‚Ø¯ÙŠÙ… ØªØºØ°ÙŠØ© Ø±Ø§Ø¬Ø¹Ø© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬"""
    try:
        feedback_enum = FeedbackType(feedback_type)
        return await adaptive_learning_manager.process_feedback(
            model_id, input_data, expected_output, actual_output, 
            feedback_enum, confidence
        )
    except ValueError:
        logging.error(f"Invalid feedback type: {feedback_type}")
        return False

async def ask_ai_question(question: str, language: str = "ar") -> Dict[str, Any]:
    """Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„ Ø¹Ù„Ù‰ Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
    try:
        lang_enum = LanguageCode(language)
        query = await adaptive_learning_manager.process_nlp_query(question, lang_enum)
        
        return {
            'success': True,
            'question': query.text,
            'answer': query.response,
            'intent': query.intent,
            'entities': query.entities,
            'confidence': query.confidence
        }
        
    except Exception as e:
        logging.error(f"Failed to process AI question: {e}")
        return {
            'success': False,
            'error': str(e)
        }

if __name__ == "__main__":
    # Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
    async def main():
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
        await initialize_adaptive_learning()
        
        # Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø³Ø¤Ø§Ù„
        result = await ask_ai_question("Ù…Ø§ Ù‡Ùˆ Ø¹Ù„Ø§Ø¬ ØªØ¨Ù‚Ø¹ Ø§Ù„Ø£ÙˆØ±Ø§Ù‚ ÙÙŠ Ø§Ù„Ø·Ù…Ø§Ø·Ù…ØŸ")
        print(f"AI Response: {result}")
        
        # Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ ØªÙ‚Ø¯ÙŠÙ… ØªØºØ°ÙŠØ© Ø±Ø§Ø¬Ø¹Ø©
        feedback_result = await submit_feedback(
            "disease_diagnosis",
            "image_data",
            "ØªØ¨Ù‚Ø¹ Ø§Ù„Ø£ÙˆØ±Ø§Ù‚",
            "ØªØ¨Ù‚Ø¹ Ø§Ù„Ø£ÙˆØ±Ø§Ù‚",
            "correct",
            0.95
        )
        print(f"Feedback submitted: {feedback_result}")
    
    asyncio.run(main())

